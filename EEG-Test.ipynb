{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# EEG SSVEP Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d726ba570e097eb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 一、Dataset类创建"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3b19d8623132aa4"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:20:02.293160100Z",
     "start_time": "2024-02-26T04:20:02.275053900Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Benchmark(Dataset):\n",
    "    \n",
    "    classes = {\n",
    "        \n",
    "    }\n",
    "    \n",
    "    stim_event_freq = [8., 8.2, 8.4, 8.6, 8.8, 9., 9.2, 9.4, 9.6, 9.8, 10., 10.2, 10.4, 10.6,\n",
    "                       10.8, 11., 11.2, 11.4, 11.6, 11.8, 12., 12.2, 12.4, 12.6, 12.8, 13., 13.2, 13.4,\n",
    "                       13.6, 13.8, 14., 14.2, 14.4, 14.6, 14.8, 15., 15.2, 15.4, 15.6, 15.8]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str = '',\n",
    "        train: bool = True,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super(Dataset).__init__()\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.subject_num = 35\n",
    "        # 采样率1000，降采样至 250Hz\n",
    "        self.samp_rate = 250\n",
    "        # 预处理滤波器设置\n",
    "        '''没看懂'''\n",
    "        self.filterB, self.filterA = self.__get_pre_filter(self.samp_rate)\n",
    "        self.data, self.pre_data, self.label = self.load_data()\n",
    "    \n",
    "    def load_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        channels = [53, 54, 55, 57, 58, 59, 61, 62, 63]\n",
    "        channels = [i - 1 for i in channels]\n",
    "        \n",
    "        if self.train:\n",
    "            # train data\n",
    "            print(\"---------- 训练数据加载 ----------\")            \n",
    "            data = np.zeros((200*self.subject_num, len(channels), 1375))\n",
    "            pre_data = np.zeros((200*self.subject_num, len(channels), 125))\n",
    "            label = np.zeros(200*self.subject_num, dtype=int)\n",
    "        else:\n",
    "            # test data\n",
    "            print(\"---------- 测试数据加载 ----------\")\n",
    "            data = np.zeros((40*self.subject_num, len(channels), 1375))\n",
    "            pre_data = np.zeros((40*self.subject_num, len(channels), 125))\n",
    "            label = np.zeros(40*self.subject_num, dtype=int)\n",
    "            \n",
    "        for sub_num in tqdm(range(1, self.subject_num+1)):\n",
    "            f = scipy.io.loadmat(self.root + f\"/S{sub_num}.mat\")\n",
    "            # print(f\"mat{sub_num}文件大小: {f['data'].shape}\")\n",
    "            for block in range(6):\n",
    "                for target in range(40):\n",
    "                    if self.train and block!=5:\n",
    "                        data[(sub_num - 1) * 200 + block * 40 + target] = f[\"data\"][channels, 125:, target, block]\n",
    "                        pre_data[(sub_num - 1) * 200 + block * 40 + target] = f[\"data\"][channels, :125, target, block]\n",
    "                        label[(sub_num - 1) * 200 + block * 40 + target] = int(target)\n",
    "                    elif not self.train and block==5:\n",
    "                        data[(sub_num - 1) * 40 + target] = f[\"data\"][channels, 125:, target, block]\n",
    "                        pre_data[(sub_num - 1) * 40 + target] = f[\"data\"][channels, :125, target, block]\n",
    "                        label[(sub_num - 1) * 40 + target] = int(target)\n",
    "        return data, pre_data, label\n",
    "    \n",
    "    def __get_pre_filter(self, samp_rate):\n",
    "        fs = samp_rate\n",
    "        f0 = 50\n",
    "        q = 35\n",
    "        b, a = signal.iircomb(f0, q, ftype='notch', fs=fs)\n",
    "        return b, a\n",
    "    \n",
    "    def __preprocess(self, data):\n",
    "        filter_data = signal.filtfilt(self.filterB, self.filterA, data)\n",
    "        return filter_data\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index) -> Tuple[Any, Any]:\n",
    "        eeg, target = self.data[index], self.label[index]\n",
    "        \n",
    "        # 滤波处理\n",
    "        eeg = self.__preprocess(eeg)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            eeg = self.transform(eeg.copy())\n",
    "            \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target.copy())\n",
    "        \n",
    "        eeg = eeg.float()\n",
    "        return eeg, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 训练数据加载 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:19<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 测试数据加载 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:19<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# torch.set_default_dtype(torch.float64)\n",
    "train_data = Benchmark(\"E:\\Datasets\\BCI\\SSVEP\\Benchmark\", train = True, \n",
    "                       transform = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ]))\n",
    "test_data = Benchmark(\"E:\\Datasets\\BCI\\SSVEP\\Benchmark\", train = False, \n",
    "                       transform = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:20:42.352178900Z",
     "start_time": "2024-02-26T04:20:03.387314500Z"
    }
   },
   "id": "b2e928e06e622fd7"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:\n",
      " 7000\n",
      "shape: torch.Size([1, 9, 1375]), type: <class 'torch.Tensor'>\n",
      "test_data:\n",
      " 1400\n",
      "shape: torch.Size([1, 9, 1375]), type: <class 'torch.Tensor'>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data:\\n\", len(train_data))\n",
    "print(f\"shape: {train_data[0][0].shape}, type: {type(train_data[0][0])}\")\n",
    "\n",
    "print(\"test_data:\\n\", len(test_data))\n",
    "print(f\"shape: {test_data[0][0].shape}, type: {type(test_data[0][0])}\")\n",
    "\n",
    "print(train_data[2][1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:20:47.316825200Z",
     "start_time": "2024-02-26T04:20:47.297820100Z"
    }
   },
   "id": "b5a91edee5bf87c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 二、EEGNet构建"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74cc6cfc51129a49"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 8, 9, 1376]             512\n",
      "       BatchNorm2d-2           [-1, 8, 9, 1376]              16\n",
      "            Conv2d-3          [-1, 16, 1, 1376]             144\n",
      "            Conv2d-4           [-1, 8, 1, 1376]             128\n",
      "DepthwiseSeparableConv2d-5           [-1, 8, 1, 1376]               0\n",
      "       BatchNorm2d-6           [-1, 8, 1, 1376]              16\n",
      "               ELU-7           [-1, 8, 1, 1376]               0\n",
      "         AvgPool2d-8            [-1, 8, 1, 344]               0\n",
      "           Dropout-9            [-1, 8, 1, 344]               0\n",
      "           Conv2d-10            [-1, 8, 1, 345]             128\n",
      "           Conv2d-11           [-1, 16, 1, 345]             128\n",
      "DepthwiseSeparableConv2d-12           [-1, 16, 1, 345]               0\n",
      "      BatchNorm2d-13           [-1, 16, 1, 345]              32\n",
      "              ELU-14           [-1, 16, 1, 345]               0\n",
      "        AvgPool2d-15            [-1, 16, 1, 43]               0\n",
      "          Dropout-16            [-1, 16, 1, 43]               0\n",
      "          Flatten-17                  [-1, 688]               0\n",
      "           Linear-18                   [-1, 40]          27,560\n",
      "          Softmax-19                   [-1, 40]               0\n",
      "================================================================\n",
      "Total params: 28,664\n",
      "Trainable params: 28,664\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 2.26\n",
      "Params size (MB): 0.11\n",
      "Estimated Total Size (MB): 2.42\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, depth_multiplier=1):\n",
    "        super(DepthwiseSeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels * depth_multiplier, kernel_size=kernel_size,\n",
    "                                   stride=(1, 1), padding=(0, 0 if kernel_size[0]>kernel_size[1] else max(kernel_size)//2), groups=in_channels, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_channels * depth_multiplier, out_channels, kernel_size=(1, 1),\n",
    "                                   stride=(1, 1), padding=(0, 0), bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes, Chans=64, Samples=128, dropoutRate=0.5, kernLength=64,\n",
    "                 F1=8, D=2, F2=16, norm_rate=0.25, dropoutType='Dropout'):\n",
    "        super(EEGNet, self).__init__()\n",
    "\n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            self.dropoutType = nn.Dropout2d\n",
    "        elif dropoutType == 'Dropout':\n",
    "            self.dropoutType = nn.Dropout\n",
    "        else:\n",
    "            raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
    "                             'or Dropout, passed as a string.')\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, F1, (1, kernLength), padding=(0, kernLength//2), bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            DepthwiseSeparableConv2d(F1, F1, kernel_size=(Chans, 1), depth_multiplier=D),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            self.dropoutType(dropoutRate)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(F1, F2, kernel_size=(1, 16), depth_multiplier=1),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8)),\n",
    "            self.dropoutType(dropoutRate)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(F2*int(np.floor((np.floor((Samples+1)/4)+1)/8)), nb_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.block1(input)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        return x\n",
    "\n",
    "learning_rate = 1e-3\n",
    "nb_classes = 40\n",
    "Chans = 9\n",
    "Samples = 1375\n",
    "model = EEGNet(nb_classes, Chans, Samples)\n",
    "model = model.to(device)\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "# print(model.state_dict()['block1.0.weight'].dtype)\n",
    "# print(device)\n",
    "print(summary(model, input_size=(1, 9, 1375), device='cuda'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:20:52.273670700Z",
     "start_time": "2024-02-26T04:20:52.012828100Z"
    }
   },
   "id": "9b344932ffcc584d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- EPOCH 1 ----------\n",
      "0.8241136074066162\t 训练次数：100, Loss：3.69568133354187\n",
      "1.420565128326416\t 训练次数：200, Loss：3.6918246746063232\n",
      "2.025568723678589\t 训练次数：300, Loss：3.6824088096618652\n",
      "2.6121058464050293\t 训练次数：400, Loss：3.658505439758301\n",
      "---------- EPOCH 2 ----------\n",
      "3.2184486389160156\t 训练次数：500, Loss：3.3628623485565186\n",
      "3.818922281265259\t 训练次数：600, Loss：3.4165542125701904\n",
      "4.409793376922607\t 训练次数：700, Loss：3.315293312072754\n",
      "4.999252080917358\t 训练次数：800, Loss：3.1848251819610596\n",
      "---------- EPOCH 3 ----------\n",
      "\n",
      "5.563786268234253\t 训练次数：900, Loss：3.342639207839966\n",
      "6.145760536193848\t 训练次数：1000, Loss：3.3122804164886475\n",
      "6.715238571166992\t 训练次数：1100, Loss：3.366192579269409\n",
      "7.293615102767944\t 训练次数：1200, Loss：3.1675117015838623\n",
      "7.869288444519043\t 训练次数：1300, Loss：3.445911407470703\n",
      "---------- EPOCH 4 ----------\n",
      "8.412700414657593\t 训练次数：1400, Loss：3.4322941303253174\n",
      "8.95007061958313\t 训练次数：1500, Loss：3.1146395206451416\n",
      "9.499636888504028\t 训练次数：1600, Loss：3.17037034034729\n",
      "10.048905849456787\t 训练次数：1700, Loss：3.2558743953704834\n",
      "---------- EPOCH 5 ----------\n",
      "10.59870457649231\t 训练次数：1800, Loss：3.102684736251831\n",
      "11.151061296463013\t 训练次数：1900, Loss：3.0119166374206543\n",
      "11.699729919433594\t 训练次数：2000, Loss：3.3211851119995117\n",
      "12.241151094436646\t 训练次数：2100, Loss：3.0903100967407227\n",
      "---------- EPOCH 6 ----------\n",
      "\n",
      "12.7729332447052\t 训练次数：2200, Loss：3.055176019668579\n",
      "13.314518451690674\t 训练次数：2300, Loss：2.9931983947753906\n",
      "13.873722553253174\t 训练次数：2400, Loss：3.247196674346924\n",
      "14.43396544456482\t 训练次数：2500, Loss：2.9720544815063477\n",
      "15.007543325424194\t 训练次数：2600, Loss：3.0846807956695557\n",
      "---------- EPOCH 7 ----------\n",
      "15.570297241210938\t 训练次数：2700, Loss：3.2866628170013428\n",
      "16.138839960098267\t 训练次数：2800, Loss：3.1078319549560547\n",
      "16.70604681968689\t 训练次数：2900, Loss：3.0503664016723633\n",
      "17.265702486038208\t 训练次数：3000, Loss：3.1067521572113037\n",
      "---------- EPOCH 8 ----------\n",
      "\n",
      "17.84904170036316\t 训练次数：3100, Loss：3.1734471321105957\n",
      "18.432990550994873\t 训练次数：3200, Loss：3.0866775512695312\n",
      "19.00852084159851\t 训练次数：3300, Loss：2.9757895469665527\n",
      "19.576961040496826\t 训练次数：3400, Loss：3.1468448638916016\n",
      "20.149229049682617\t 训练次数：3500, Loss：2.994126319885254\n",
      "---------- EPOCH 9 ----------\n",
      "20.73979878425598\t 训练次数：3600, Loss：3.071685791015625\n",
      "21.313004970550537\t 训练次数：3700, Loss：2.9142415523529053\n",
      "21.893361806869507\t 训练次数：3800, Loss：3.057769298553467\n",
      "22.47278904914856\t 训练次数：3900, Loss：3.142874002456665\n",
      "---------- EPOCH 10 ----------\n",
      "23.050809860229492\t 训练次数：4000, Loss：2.9856269359588623\n",
      "23.624848127365112\t 训练次数：4100, Loss：3.0594727993011475\n",
      "24.215673208236694\t 训练次数：4200, Loss：3.117633581161499\n",
      "24.80285382270813\t 训练次数：4300, Loss：2.8218610286712646\n",
      "---------- EPOCH 11 ----------\n",
      "\n",
      "25.371490240097046\t 训练次数：4400, Loss：2.998093366622925\n",
      "25.92788290977478\t 训练次数：4500, Loss：2.995107889175415\n",
      "26.481305599212646\t 训练次数：4600, Loss：3.1153838634490967\n",
      "27.030988931655884\t 训练次数：4700, Loss：2.9327008724212646\n",
      "27.592524528503418\t 训练次数：4800, Loss：2.9374966621398926\n",
      "---------- EPOCH 12 ----------\n",
      "28.138570070266724\t 训练次数：4900, Loss：2.8450796604156494\n",
      "28.7111656665802\t 训练次数：5000, Loss：3.1328203678131104\n",
      "29.267690658569336\t 训练次数：5100, Loss：3.048942804336548\n",
      "29.814852476119995\t 训练次数：5200, Loss：3.141753673553467\n",
      "---------- EPOCH 13 ----------\n",
      "30.35434055328369\t 训练次数：5300, Loss：3.0191848278045654\n",
      "30.88556432723999\t 训练次数：5400, Loss：2.973623514175415\n",
      "31.420557737350464\t 训练次数：5500, Loss：3.130657911300659\n",
      "31.969501972198486\t 训练次数：5600, Loss：3.258699893951416\n",
      "---------- EPOCH 14 ----------\n",
      "\n",
      "32.50787305831909\t 训练次数：5700, Loss：3.0611119270324707\n",
      "33.04680156707764\t 训练次数：5800, Loss：2.9511826038360596\n",
      "33.582480669021606\t 训练次数：5900, Loss：2.8817081451416016\n",
      "34.13181734085083\t 训练次数：6000, Loss：3.0659189224243164\n",
      "34.70740246772766\t 训练次数：6100, Loss：3.0912909507751465\n",
      "---------- EPOCH 15 ----------\n",
      "35.27678823471069\t 训练次数：6200, Loss：2.870492935180664\n",
      "35.83446788787842\t 训练次数：6300, Loss：2.9434525966644287\n",
      "36.390058517456055\t 训练次数：6400, Loss：2.898453712463379\n",
      "36.93854808807373\t 训练次数：6500, Loss：2.9282240867614746\n",
      "---------- EPOCH 16 ----------\n",
      "\n",
      "37.47760343551636\t 训练次数：6600, Loss：2.9336330890655518\n",
      "38.027236461639404\t 训练次数：6700, Loss：3.1304919719696045\n",
      "38.58481431007385\t 训练次数：6800, Loss：3.0128543376922607\n",
      "39.13854455947876\t 训练次数：6900, Loss：2.8728442192077637\n",
      "39.68108916282654\t 训练次数：7000, Loss：3.0574941635131836\n",
      "---------- EPOCH 17 ----------\n",
      "40.22420287132263\t 训练次数：7100, Loss：3.091304302215576\n",
      "40.78425693511963\t 训练次数：7200, Loss：3.0364503860473633\n",
      "41.35036015510559\t 训练次数：7300, Loss：2.9823029041290283\n",
      "41.925028800964355\t 训练次数：7400, Loss：2.954481601715088\n",
      "---------- EPOCH 18 ----------\n",
      "42.51222467422485\t 训练次数：7500, Loss：2.919877767562866\n",
      "43.1111056804657\t 训练次数：7600, Loss：2.9326417446136475\n",
      "43.6729097366333\t 训练次数：7700, Loss：2.902134895324707\n",
      "44.236443519592285\t 训练次数：7800, Loss：3.0319340229034424\n",
      "---------- EPOCH 19 ----------\n",
      "\n",
      "44.81301140785217\t 训练次数：7900, Loss：2.8612098693847656\n",
      "45.388543128967285\t 训练次数：8000, Loss：3.0847442150115967\n",
      "45.958746671676636\t 训练次数：8100, Loss：3.043628454208374\n",
      "46.53875756263733\t 训练次数：8200, Loss：3.0214691162109375\n",
      "47.11708211898804\t 训练次数：8300, Loss：2.910719156265259\n",
      "---------- EPOCH 20 ----------\n",
      "47.701716899871826\t 训练次数：8400, Loss：2.915083646774292\n",
      "48.28000020980835\t 训练次数：8500, Loss：2.8846962451934814\n",
      "48.855254888534546\t 训练次数：8600, Loss：2.979771852493286\n",
      "49.44169998168945\t 训练次数：8700, Loss：2.8634328842163086\n",
      "---------- EPOCH 21 ----------\n",
      "50.0219042301178\t 训练次数：8800, Loss：2.978156089782715\n",
      "50.58831548690796\t 训练次数：8900, Loss：2.9317593574523926\n",
      "51.14175629615784\t 训练次数：9000, Loss：2.9315199851989746\n",
      "51.69461917877197\t 训练次数：9100, Loss：2.909882068634033\n",
      "---------- EPOCH 22 ----------\n",
      "\n",
      "52.24146890640259\t 训练次数：9200, Loss：2.907095193862915\n",
      "52.794612884521484\t 训练次数：9300, Loss：2.9344937801361084\n",
      "53.334062814712524\t 训练次数：9400, Loss：2.9217376708984375\n",
      "53.88023257255554\t 训练次数：9500, Loss：3.048442840576172\n",
      "54.447996616363525\t 训练次数：9600, Loss：2.801284074783325\n",
      "---------- EPOCH 23 ----------\n",
      "55.00148677825928\t 训练次数：9700, Loss：2.8672964572906494\n",
      "55.575634479522705\t 训练次数：9800, Loss：2.9823997020721436\n",
      "56.11200761795044\t 训练次数：9900, Loss：3.0165932178497314\n",
      "56.68386435508728\t 训练次数：10000, Loss：2.9070887565612793\n",
      "---------- EPOCH 24 ----------\n",
      "\n",
      "57.25461173057556\t 训练次数：10100, Loss：2.8481996059417725\n",
      "57.809821367263794\t 训练次数：10200, Loss：2.925827980041504\n",
      "58.35620880126953\t 训练次数：10300, Loss：2.9727494716644287\n",
      "58.90863919258118\t 训练次数：10400, Loss：2.99834942817688\n",
      "59.4596266746521\t 训练次数：10500, Loss：2.905143976211548\n",
      "---------- EPOCH 25 ----------\n",
      "60.02904224395752\t 训练次数：10600, Loss：2.844578504562378\n",
      "60.59263372421265\t 训练次数：10700, Loss：2.8270537853240967\n",
      "61.17364716529846\t 训练次数：10800, Loss：2.8649473190307617\n",
      "61.78041887283325\t 训练次数：10900, Loss：2.857586622238159\n",
      "---------- EPOCH 26 ----------\n",
      "62.36125993728638\t 训练次数：11000, Loss：3.1422173976898193\n",
      "62.95280122756958\t 训练次数：11100, Loss：2.94631290435791\n",
      "63.614861488342285\t 训练次数：11200, Loss：2.870314598083496\n",
      "64.20197463035583\t 训练次数：11300, Loss：3.112027645111084\n",
      "---------- EPOCH 27 ----------\n",
      "\n",
      "64.81797456741333\t 训练次数：11400, Loss：2.8678579330444336\n",
      "65.3771424293518\t 训练次数：11500, Loss：2.737494468688965\n",
      "65.97069931030273\t 训练次数：11600, Loss：2.9537341594696045\n",
      "66.5780975818634\t 训练次数：11700, Loss：3.0247044563293457\n",
      "67.19131755828857\t 训练次数：11800, Loss：2.8788843154907227\n",
      "---------- EPOCH 28 ----------\n",
      "67.80078053474426\t 训练次数：11900, Loss：2.942354440689087\n",
      "68.90409851074219\t 训练次数：12000, Loss：2.9543073177337646\n",
      "70.11596655845642\t 训练次数：12100, Loss：2.948082447052002\n",
      "71.28424572944641\t 训练次数：12200, Loss：2.909752368927002\n",
      "---------- EPOCH 29 ----------\n",
      "72.45814204216003\t 训练次数：12300, Loss：2.8512697219848633\n",
      "73.61325764656067\t 训练次数：12400, Loss：2.826441764831543\n",
      "74.79859185218811\t 训练次数：12500, Loss：2.9388163089752197\n",
      "75.98908352851868\t 训练次数：12600, Loss：2.9874978065490723\n",
      "77.19393610954285\t 训练次数：12700, Loss：2.8542768955230713\n",
      "---------- EPOCH 30 ----------\n",
      "78.3771288394928\t 训练次数：12800, Loss：2.7470932006835938\n",
      "79.57351970672607\t 训练次数：12900, Loss：2.8417606353759766\n",
      "80.75733208656311\t 训练次数：13000, Loss：3.0048279762268066\n",
      "81.95277667045593\t 训练次数：13100, Loss：2.9607691764831543\n",
      "---------- EPOCH 31 ----------\n",
      "83.17385745048523\t 训练次数：13200, Loss：3.0834286212921143\n",
      "84.36339449882507\t 训练次数：13300, Loss：2.8101987838745117\n",
      "85.54953193664551\t 训练次数：13400, Loss：2.7560267448425293\n",
      "86.8454601764679\t 训练次数：13500, Loss：2.9304442405700684\n",
      "---------- EPOCH 32 ----------\n",
      "88.05109667778015\t 训练次数：13600, Loss：3.022921323776245\n",
      "89.26094841957092\t 训练次数：13700, Loss：3.029811143875122\n",
      "90.4677562713623\t 训练次数：13800, Loss：3.0648646354675293\n",
      "91.70571565628052\t 训练次数：13900, Loss：2.846839666366577\n",
      "92.93245077133179\t 训练次数：14000, Loss：2.846071243286133\n",
      "---------- EPOCH 33 ----------\n",
      "94.14744162559509\t 训练次数：14100, Loss：2.9946541786193848\n",
      "95.39647245407104\t 训练次数：14200, Loss：2.9139294624328613\n",
      "96.58427739143372\t 训练次数：14300, Loss：3.004761219024658\n",
      "97.79664874076843\t 训练次数：14400, Loss：2.902750253677368\n",
      "---------- EPOCH 34 ----------\n",
      "98.97822856903076\t 训练次数：14500, Loss：2.8321893215179443\n",
      "100.17696499824524\t 训练次数：14600, Loss：3.056328773498535\n",
      "101.37363290786743\t 训练次数：14700, Loss：2.862232208251953\n",
      "102.62987303733826\t 训练次数：14800, Loss：2.8008956909179688\n",
      "---------- EPOCH 35 ----------\n",
      "\n",
      "103.77664470672607\t 训练次数：14900, Loss：2.937133312225342\n",
      "104.95778155326843\t 训练次数：15000, Loss：2.8202013969421387\n",
      "106.13013958930969\t 训练次数：15100, Loss：2.862579584121704\n",
      "107.28054523468018\t 训练次数：15200, Loss：2.993377208709717\n",
      "108.47473812103271\t 训练次数：15300, Loss：2.8670687675476074\n",
      "---------- EPOCH 36 ----------\n",
      "109.67268252372742\t 训练次数：15400, Loss：2.7655704021453857\n",
      "110.87071347236633\t 训练次数：15500, Loss：2.9735987186431885\n",
      "112.06490015983582\t 训练次数：15600, Loss：2.8907458782196045\n",
      "113.23262310028076\t 训练次数：15700, Loss：2.87764573097229\n",
      "---------- EPOCH 37 ----------\n",
      "114.39541935920715\t 训练次数：15800, Loss：3.006030321121216\n",
      "115.51739120483398\t 训练次数：15900, Loss：2.8516783714294434\n",
      "116.66087102890015\t 训练次数：16000, Loss：2.8387856483459473\n",
      "117.91714525222778\t 训练次数：16100, Loss：2.8831937313079834\n",
      "119.10872316360474\t 训练次数：16200, Loss：2.879539728164673\n",
      "---------- EPOCH 38 ----------\n",
      "120.32034015655518\t 训练次数：16300, Loss：2.883906126022339\n",
      "121.5097005367279\t 训练次数：16400, Loss：2.819383144378662\n",
      "122.70942735671997\t 训练次数：16500, Loss：2.993898391723633\n",
      "123.88226532936096\t 训练次数：16600, Loss：2.9358901977539062\n",
      "---------- EPOCH 39 ----------\n",
      "125.01939702033997\t 训练次数：16700, Loss：3.025498390197754\n",
      "126.19217610359192\t 训练次数：16800, Loss：2.862520694732666\n",
      "127.36423993110657\t 训练次数：16900, Loss：2.9393577575683594\n",
      "128.50860476493835\t 训练次数：17000, Loss：2.8246042728424072\n",
      "---------- EPOCH 40 ----------\n",
      "\n",
      "129.7016921043396\t 训练次数：17100, Loss：3.077441692352295\n",
      "130.87066102027893\t 训练次数：17200, Loss：2.902212381362915\n",
      "131.97575569152832\t 训练次数：17300, Loss：2.963775873184204\n",
      "133.09663820266724\t 训练次数：17400, Loss：2.8852086067199707\n",
      "134.25324726104736\t 训练次数：17500, Loss：2.904383420944214\n",
      "---------- EPOCH 41 ----------\n",
      "135.41231441497803\t 训练次数：17600, Loss：2.794534683227539\n",
      "136.55127334594727\t 训练次数：17700, Loss：2.8782153129577637\n",
      "137.6904513835907\t 训练次数：17800, Loss：2.986826181411743\n",
      "138.8180959224701\t 训练次数：17900, Loss：2.8897671699523926\n",
      "---------- EPOCH 42 ----------\n",
      "139.93447017669678\t 训练次数：18000, Loss：2.8976340293884277\n",
      "141.0811731815338\t 训练次数：18100, Loss：2.8997042179107666\n",
      "142.25662422180176\t 训练次数：18200, Loss：2.893686056137085\n",
      "143.49474549293518\t 训练次数：18300, Loss：2.8586924076080322\n",
      "---------- EPOCH 43 ----------\n",
      "\n",
      "144.71878004074097\t 训练次数：18400, Loss：2.926072359085083\n",
      "145.87292957305908\t 训练次数：18500, Loss：3.0317585468292236\n",
      "147.10909056663513\t 训练次数：18600, Loss：2.902858257293701\n",
      "148.32380056381226\t 训练次数：18700, Loss：2.936363697052002\n",
      "149.54191303253174\t 训练次数：18800, Loss：2.845525026321411\n",
      "---------- EPOCH 44 ----------\n",
      "150.72952222824097\t 训练次数：18900, Loss：3.0339772701263428\n",
      "151.93970108032227\t 训练次数：19000, Loss：2.9342265129089355\n",
      "153.0848376750946\t 训练次数：19100, Loss：3.0192384719848633\n",
      "154.28049111366272\t 训练次数：19200, Loss：2.917421579360962\n",
      "---------- EPOCH 45 ----------\n",
      "155.47604274749756\t 训练次数：19300, Loss：2.932281017303467\n",
      "156.6596827507019\t 训练次数：19400, Loss：2.8172900676727295\n",
      "157.86218643188477\t 训练次数：19500, Loss：2.9345972537994385\n",
      "159.09647727012634\t 训练次数：19600, Loss：2.908874273300171\n",
      "160.32832264900208\t 训练次数：19700, Loss：2.956017255783081\n",
      "---------- EPOCH 46 ----------\n",
      "161.54247164726257\t 训练次数：19800, Loss：2.895665407180786\n",
      "162.71116852760315\t 训练次数：19900, Loss：2.8887860774993896\n",
      "163.8958010673523\t 训练次数：20000, Loss：2.9076344966888428\n",
      "165.0898518562317\t 训练次数：20100, Loss：2.882490873336792\n",
      "---------- EPOCH 47 ----------\n",
      "166.25861859321594\t 训练次数：20200, Loss：2.7535598278045654\n",
      "167.41685914993286\t 训练次数：20300, Loss：2.865734338760376\n",
      "168.6051688194275\t 训练次数：20400, Loss：2.801882028579712\n",
      "169.83525848388672\t 训练次数：20500, Loss：2.843595504760742\n",
      "---------- EPOCH 48 ----------\n",
      "\n",
      "170.98470902442932\t 训练次数：20600, Loss：2.89414119720459\n",
      "172.18508863449097\t 训练次数：20700, Loss：2.9959208965301514\n",
      "173.38516855239868\t 训练次数：20800, Loss：2.794973134994507\n",
      "174.55309581756592\t 训练次数：20900, Loss：2.861513614654541\n",
      "175.71089363098145\t 训练次数：21000, Loss：3.0724987983703613\n",
      "---------- EPOCH 49 ----------\n",
      "176.8790638446808\t 训练次数：21100, Loss：2.876770257949829\n",
      "178.06215023994446\t 训练次数：21200, Loss：2.8000853061676025\n",
      "179.261785030365\t 训练次数：21300, Loss：2.893197774887085\n",
      "180.44145131111145\t 训练次数：21400, Loss：2.8076858520507812\n",
      "---------- EPOCH 50 ----------\n",
      "181.627179145813\t 训练次数：21500, Loss：2.7964062690734863\n",
      "182.8005301952362\t 训练次数：21600, Loss：2.8738698959350586\n",
      "183.97222113609314\t 训练次数：21700, Loss：2.8224868774414062\n",
      "185.16245555877686\t 训练次数：21800, Loss：2.798978090286255\n",
      "186.3487105369568\t 训练次数：21900, Loss：2.8496649265289307\n",
      "---------- EPOCH 51 ----------\n",
      "187.53619074821472\t 训练次数：22000, Loss：2.953796148300171\n",
      "188.70322799682617\t 训练次数：22100, Loss：3.0900862216949463\n",
      "189.87127089500427\t 训练次数：22200, Loss：2.8070578575134277\n",
      "191.0749146938324\t 训练次数：22300, Loss：2.9641551971435547\n",
      "---------- EPOCH 52 ----------\n",
      "192.28279376029968\t 训练次数：22400, Loss：2.8581809997558594\n",
      "193.58223056793213\t 训练次数：22500, Loss：2.8457441329956055\n",
      "194.79855847358704\t 训练次数：22600, Loss：3.0427613258361816\n",
      "196.00704908370972\t 训练次数：22700, Loss：2.8795435428619385\n",
      "---------- EPOCH 53 ----------\n",
      "197.185307264328\t 训练次数：22800, Loss：2.795822858810425\n",
      "198.38258624076843\t 训练次数：22900, Loss：2.8506832122802734\n",
      "199.60125303268433\t 训练次数：23000, Loss：2.9080400466918945\n",
      "200.79839158058167\t 训练次数：23100, Loss：2.8465139865875244\n",
      "201.9871814250946\t 训练次数：23200, Loss：2.8284912109375\n",
      "---------- EPOCH 54 ----------\n",
      "203.20196294784546\t 训练次数：23300, Loss：2.8138062953948975\n",
      "204.42518615722656\t 训练次数：23400, Loss：2.8940465450286865\n",
      "205.764657497406\t 训练次数：23500, Loss：2.8556067943573\n",
      "207.00745153427124\t 训练次数：23600, Loss：2.8422389030456543\n",
      "---------- EPOCH 55 ----------\n",
      "208.1770224571228\t 训练次数：23700, Loss：2.7737855911254883\n",
      "209.3703737258911\t 训练次数：23800, Loss：2.980255603790283\n",
      "210.54142451286316\t 训练次数：23900, Loss：2.8688650131225586\n",
      "211.72472023963928\t 训练次数：24000, Loss：2.926862955093384\n",
      "---------- EPOCH 56 ----------\n",
      "\n",
      "212.9317982196808\t 训练次数：24100, Loss：2.930575132369995\n",
      "214.16154789924622\t 训练次数：24200, Loss：2.8487534523010254\n",
      "215.38633823394775\t 训练次数：24300, Loss：2.8229427337646484\n",
      "216.60738348960876\t 训练次数：24400, Loss：2.9771296977996826\n",
      "217.8354890346527\t 训练次数：24500, Loss：2.906729221343994\n",
      "---------- EPOCH 57 ----------\n",
      "219.05650281906128\t 训练次数：24600, Loss：2.843008518218994\n",
      "220.2669472694397\t 训练次数：24700, Loss：2.802032947540283\n",
      "221.45067715644836\t 训练次数：24800, Loss：2.819643259048462\n",
      "222.66007089614868\t 训练次数：24900, Loss：2.830449104309082\n",
      "---------- EPOCH 58 ----------\n",
      "223.86704230308533\t 训练次数：25000, Loss：2.9366769790649414\n",
      "225.0652916431427\t 训练次数：25100, Loss：2.885089874267578\n",
      "226.2644340991974\t 训练次数：25200, Loss：2.915437698364258\n",
      "227.44660711288452\t 训练次数：25300, Loss：2.8606457710266113\n",
      "228.62381649017334\t 训练次数：25400, Loss：2.8212361335754395\n",
      "---------- EPOCH 59 ----------\n",
      "229.79894971847534\t 训练次数：25500, Loss：2.739365577697754\n",
      "231.02884030342102\t 训练次数：25600, Loss：2.794412136077881\n",
      "232.25363111495972\t 训练次数：25700, Loss：2.9099953174591064\n",
      "233.51128602027893\t 训练次数：25800, Loss：3.007028341293335\n",
      "---------- EPOCH 60 ----------\n",
      "234.71534824371338\t 训练次数：25900, Loss：2.864034652709961\n",
      "235.8885850906372\t 训练次数：26000, Loss：2.90732741355896\n",
      "237.10754370689392\t 训练次数：26100, Loss：2.7649168968200684\n",
      "238.28506922721863\t 训练次数：26200, Loss：2.8178536891937256\n",
      "---------- EPOCH 61 ----------\n",
      "239.5164773464203\t 训练次数：26300, Loss：2.7817976474761963\n",
      "240.73857474327087\t 训练次数：26400, Loss：2.845935583114624\n",
      "241.96359992027283\t 训练次数：26500, Loss：2.8149526119232178\n",
      "243.13503432273865\t 训练次数：26600, Loss：2.9763307571411133\n",
      "244.3766314983368\t 训练次数：26700, Loss：2.8043212890625\n",
      "---------- EPOCH 62 ----------\n",
      "245.60528707504272\t 训练次数：26800, Loss：2.866332530975342\n",
      "246.795254945755\t 训练次数：26900, Loss：2.856278896331787\n",
      "247.9860429763794\t 训练次数：27000, Loss：2.791656970977783\n",
      "249.19369435310364\t 训练次数：27100, Loss：2.9381706714630127\n",
      "---------- EPOCH 63 ----------\n",
      "250.4227225780487\t 训练次数：27200, Loss：2.8466949462890625\n",
      "251.64320039749146\t 训练次数：27300, Loss：2.783278226852417\n",
      "252.87699437141418\t 训练次数：27400, Loss：2.8459832668304443\n",
      "254.08589339256287\t 训练次数：27500, Loss：2.82658314704895\n",
      "---------- EPOCH 64 ----------\n",
      "\n",
      "255.30326581001282\t 训练次数：27600, Loss：2.85428786277771\n",
      "256.49964809417725\t 训练次数：27700, Loss：3.0413904190063477\n",
      "257.76344752311707\t 训练次数：27800, Loss：2.8122997283935547\n",
      "258.9754295349121\t 训练次数：27900, Loss：2.8686366081237793\n",
      "260.17603302001953\t 训练次数：28000, Loss：2.8892266750335693\n",
      "---------- EPOCH 65 ----------\n",
      "261.3851761817932\t 训练次数：28100, Loss：2.7599189281463623\n",
      "262.58963990211487\t 训练次数：28200, Loss：2.891965866088867\n",
      "263.77995800971985\t 训练次数：28300, Loss：2.825270652770996\n",
      "264.95176458358765\t 训练次数：28400, Loss：2.7984349727630615\n",
      "---------- EPOCH 66 ----------\n",
      "266.1554310321808\t 训练次数：28500, Loss：2.799776315689087\n",
      "267.356027841568\t 训练次数：28600, Loss：2.891374111175537\n",
      "268.57607913017273\t 训练次数：28700, Loss：2.8935959339141846\n",
      "269.7684805393219\t 训练次数：28800, Loss：2.811100721359253\n",
      "270.98968052864075\t 训练次数：28900, Loss：2.8615593910217285\n",
      "---------- EPOCH 67 ----------\n",
      "272.203293800354\t 训练次数：29000, Loss：2.881761312484741\n",
      "273.44743037223816\t 训练次数：29100, Loss：2.795377254486084\n",
      "274.6712188720703\t 训练次数：29200, Loss：2.9068896770477295\n",
      "275.88696479797363\t 训练次数：29300, Loss：2.8673834800720215\n",
      "---------- EPOCH 68 ----------\n",
      "277.11053133010864\t 训练次数：29400, Loss：2.8733935356140137\n",
      "278.29067039489746\t 训练次数：29500, Loss：2.8149232864379883\n",
      "279.4700710773468\t 训练次数：29600, Loss：2.933985471725464\n",
      "280.6762080192566\t 训练次数：29700, Loss：2.943079948425293\n",
      "---------- EPOCH 69 ----------\n",
      "\n",
      "281.87478947639465\t 训练次数：29800, Loss：2.7671852111816406\n",
      "283.08138394355774\t 训练次数：29900, Loss：2.9257454872131348\n",
      "284.2611620426178\t 训练次数：30000, Loss：2.817648410797119\n",
      "285.465927362442\t 训练次数：30100, Loss：2.824282646179199\n",
      "286.6758244037628\t 训练次数：30200, Loss：2.7966291904449463\n",
      "---------- EPOCH 70 ----------\n",
      "287.87998247146606\t 训练次数：30300, Loss：2.8638288974761963\n",
      "289.05436182022095\t 训练次数：30400, Loss：2.7924909591674805\n",
      "290.2607796192169\t 训练次数：30500, Loss：2.7340431213378906\n",
      "291.469603061676\t 训练次数：30600, Loss：2.9882097244262695\n",
      "---------- EPOCH 71 ----------\n",
      "292.694043636322\t 训练次数：30700, Loss：2.8148322105407715\n",
      "293.8910710811615\t 训练次数：30800, Loss：2.883180856704712\n",
      "295.0999393463135\t 训练次数：30900, Loss：2.978297710418701\n",
      "296.30022072792053\t 训练次数：31000, Loss：2.865762233734131\n",
      "---------- EPOCH 72 ----------\n",
      "\n",
      "297.5047435760498\t 训练次数：31100, Loss：2.8765735626220703\n",
      "298.6836893558502\t 训练次数：31200, Loss：2.874354839324951\n",
      "299.84808230400085\t 训练次数：31300, Loss：2.837355375289917\n",
      "301.0159306526184\t 训练次数：31400, Loss：3.0449087619781494\n",
      "302.19236063957214\t 训练次数：31500, Loss：2.8390326499938965\n",
      "---------- EPOCH 73 ----------\n",
      "303.36937522888184\t 训练次数：31600, Loss：2.8106822967529297\n",
      "304.5757727622986\t 训练次数：31700, Loss：2.7785348892211914\n",
      "305.75062799453735\t 训练次数：31800, Loss：2.7446088790893555\n",
      "306.93570923805237\t 训练次数：31900, Loss：2.8316192626953125\n",
      "---------- EPOCH 74 ----------\n",
      "308.13343954086304\t 训练次数：32000, Loss：2.880901575088501\n",
      "309.2992651462555\t 训练次数：32100, Loss：2.825496196746826\n",
      "310.4973545074463\t 训练次数：32200, Loss：2.894601821899414\n",
      "311.70983266830444\t 训练次数：32300, Loss：2.866865396499634\n",
      "312.90146470069885\t 训练次数：32400, Loss：2.824078321456909\n",
      "---------- EPOCH 75 ----------\n",
      "314.1064043045044\t 训练次数：32500, Loss：2.979963779449463\n",
      "315.33510184288025\t 训练次数：32600, Loss：2.8907651901245117\n",
      "316.57099986076355\t 训练次数：32700, Loss：2.8368923664093018\n",
      "317.75800108909607\t 训练次数：32800, Loss：2.909475326538086\n",
      "---------- EPOCH 76 ----------\n",
      "318.973530292511\t 训练次数：32900, Loss：2.920469284057617\n",
      "320.16126132011414\t 训练次数：33000, Loss：2.9951179027557373\n",
      "321.45006012916565\t 训练次数：33100, Loss：2.8048415184020996\n",
      "322.6774961948395\t 训练次数：33200, Loss：2.7367899417877197\n",
      "---------- EPOCH 77 ----------\n",
      "\n",
      "323.90374517440796\t 训练次数：33300, Loss：2.8323757648468018\n",
      "325.07371616363525\t 训练次数：33400, Loss：2.811187982559204\n",
      "326.2629179954529\t 训练次数：33500, Loss：3.0818493366241455\n",
      "327.48808574676514\t 训练次数：33600, Loss：2.8314590454101562\n",
      "328.7072124481201\t 训练次数：33700, Loss：2.7579827308654785\n",
      "---------- EPOCH 78 ----------\n",
      "329.918582201004\t 训练次数：33800, Loss：2.8378429412841797\n",
      "331.1216127872467\t 训练次数：33900, Loss：2.7965219020843506\n",
      "332.352511882782\t 训练次数：34000, Loss：2.896953582763672\n",
      "333.5440745353699\t 训练次数：34100, Loss：2.823742628097534\n",
      "---------- EPOCH 79 ----------\n",
      "334.80751514434814\t 训练次数：34200, Loss：2.7360641956329346\n",
      "336.0447373390198\t 训练次数：34300, Loss：2.849193572998047\n",
      "337.2513339519501\t 训练次数：34400, Loss：2.793665885925293\n",
      "338.43991923332214\t 训练次数：34500, Loss：2.843682289123535\n",
      "339.6041679382324\t 训练次数：34600, Loss：2.8417911529541016\n",
      "---------- EPOCH 80 ----------\n",
      "340.80140948295593\t 训练次数：34700, Loss：2.8330092430114746\n",
      "341.96671962738037\t 训练次数：34800, Loss：2.8506791591644287\n",
      "343.1380729675293\t 训练次数：34900, Loss：2.862769603729248\n",
      "344.3248088359833\t 训练次数：35000, Loss：3.0032989978790283\n",
      "---------- EPOCH 81 ----------\n",
      "345.5188663005829\t 训练次数：35100, Loss：2.8059070110321045\n",
      "346.7144091129303\t 训练次数：35200, Loss：2.9097073078155518\n",
      "347.9114592075348\t 训练次数：35300, Loss：2.7722651958465576\n",
      "349.1227867603302\t 训练次数：35400, Loss：2.948481798171997\n",
      "---------- EPOCH 82 ----------\n",
      "350.31045413017273\t 训练次数：35500, Loss：2.749044895172119\n",
      "351.4597964286804\t 训练次数：35600, Loss：2.9428160190582275\n",
      "352.65504217147827\t 训练次数：35700, Loss：2.9175617694854736\n",
      "353.86753726005554\t 训练次数：35800, Loss：2.8173460960388184\n",
      "355.0063474178314\t 训练次数：35900, Loss：3.041980504989624\n",
      "---------- EPOCH 83 ----------\n",
      "356.19200897216797\t 训练次数：36000, Loss：2.8130722045898438\n",
      "357.3883981704712\t 训练次数：36100, Loss：2.8596556186676025\n",
      "358.5634272098541\t 训练次数：36200, Loss：2.747039556503296\n",
      "359.76758551597595\t 训练次数：36300, Loss：2.8496954441070557\n",
      "---------- EPOCH 84 ----------\n",
      "360.9128966331482\t 训练次数：36400, Loss：2.8791964054107666\n",
      "362.10654282569885\t 训练次数：36500, Loss：2.9005939960479736\n",
      "363.3126537799835\t 训练次数：36600, Loss：2.740184783935547\n",
      "364.501571893692\t 训练次数：36700, Loss：2.861130714416504\n",
      "---------- EPOCH 85 ----------\n",
      "\n",
      "365.7247226238251\t 训练次数：36800, Loss：2.8667421340942383\n",
      "366.9150643348694\t 训练次数：36900, Loss：2.955827236175537\n",
      "368.14654326438904\t 训练次数：37000, Loss：2.818044424057007\n",
      "369.3341956138611\t 训练次数：37100, Loss：2.8329596519470215\n",
      "370.54078221321106\t 训练次数：37200, Loss：2.7769291400909424\n",
      "---------- EPOCH 86 ----------\n",
      "371.7031743526459\t 训练次数：37300, Loss：2.9032771587371826\n",
      "372.8985695838928\t 训练次数：37400, Loss：2.849663734436035\n",
      "374.08351612091064\t 训练次数：37500, Loss：2.9931845664978027\n",
      "375.2558403015137\t 训练次数：37600, Loss：2.802527904510498\n",
      "---------- EPOCH 87 ----------\n",
      "376.40051531791687\t 训练次数：37700, Loss：2.963672161102295\n",
      "377.58996081352234\t 训练次数：37800, Loss：2.735262155532837\n",
      "378.7702922821045\t 训练次数：37900, Loss：2.8034861087799072\n",
      "379.95762395858765\t 训练次数：38000, Loss：2.880945920944214\n",
      "381.13454699516296\t 训练次数：38100, Loss：2.9432737827301025\n",
      "---------- EPOCH 88 ----------\n",
      "382.32294273376465\t 训练次数：38200, Loss：2.860983371734619\n",
      "383.50733757019043\t 训练次数：38300, Loss：2.891580581665039\n",
      "384.710480928421\t 训练次数：38400, Loss：2.8595516681671143\n",
      "385.88651418685913\t 训练次数：38500, Loss：2.792728900909424\n",
      "---------- EPOCH 89 ----------\n",
      "387.0756092071533\t 训练次数：38600, Loss：2.789985418319702\n",
      "388.2594528198242\t 训练次数：38700, Loss：2.7856836318969727\n",
      "389.422486782074\t 训练次数：38800, Loss：2.9664390087127686\n",
      "390.5657651424408\t 训练次数：38900, Loss：2.9358623027801514\n",
      "---------- EPOCH 90 ----------\n",
      "\n",
      "391.7046949863434\t 训练次数：39000, Loss：2.840064287185669\n",
      "392.8694221973419\t 训练次数：39100, Loss：2.853944778442383\n",
      "394.0633661746979\t 训练次数：39200, Loss：2.7797038555145264\n",
      "395.239453792572\t 训练次数：39300, Loss：2.732455253601074\n",
      "396.41703391075134\t 训练次数：39400, Loss：2.7492566108703613\n",
      "---------- EPOCH 91 ----------\n",
      "397.63631319999695\t 训练次数：39500, Loss：2.8392274379730225\n",
      "398.8108172416687\t 训练次数：39600, Loss：2.901644706726074\n",
      "400.02913308143616\t 训练次数：39700, Loss：2.765289783477783\n",
      "401.20557832717896\t 训练次数：39800, Loss：2.8385074138641357\n",
      "---------- EPOCH 92 ----------\n",
      "402.4144973754883\t 训练次数：39900, Loss：2.922605514526367\n",
      "403.6165702342987\t 训练次数：40000, Loss：2.8522181510925293\n",
      "404.7917423248291\t 训练次数：40100, Loss：2.78316593170166\n",
      "405.99329566955566\t 训练次数：40200, Loss：2.9151217937469482\n",
      "---------- EPOCH 93 ----------\n",
      "\n",
      "407.1964523792267\t 训练次数：40300, Loss：2.76055908203125\n",
      "408.361172914505\t 训练次数：40400, Loss：2.774381637573242\n",
      "409.5215907096863\t 训练次数：40500, Loss：2.7632501125335693\n",
      "410.71983575820923\t 训练次数：40600, Loss：2.8789806365966797\n",
      "411.90427446365356\t 训练次数：40700, Loss：2.852388858795166\n",
      "---------- EPOCH 94 ----------\n",
      "413.08421087265015\t 训练次数：40800, Loss：2.759575128555298\n",
      "414.2732651233673\t 训练次数：40900, Loss：2.87349009513855\n",
      "415.4710328578949\t 训练次数：41000, Loss：2.8127684593200684\n",
      "416.6602964401245\t 训练次数：41100, Loss：2.913994073867798\n",
      "---------- EPOCH 95 ----------\n",
      "417.85691833496094\t 训练次数：41200, Loss：2.8553264141082764\n",
      "419.0444087982178\t 训练次数：41300, Loss：2.808915138244629\n",
      "420.21278262138367\t 训练次数：41400, Loss：2.9484236240386963\n",
      "421.37528371810913\t 训练次数：41500, Loss：2.9298765659332275\n",
      "422.55752205848694\t 训练次数：41600, Loss：2.863600730895996\n",
      "---------- EPOCH 96 ----------\n",
      "423.73193621635437\t 训练次数：41700, Loss：2.934258460998535\n",
      "424.95824813842773\t 训练次数：41800, Loss：2.891319990158081\n",
      "426.1319019794464\t 训练次数：41900, Loss：2.803764581680298\n",
      "427.3176865577698\t 训练次数：42000, Loss：2.9338221549987793\n",
      "---------- EPOCH 97 ----------\n",
      "428.52124071121216\t 训练次数：42100, Loss：2.7476918697357178\n",
      "429.7173309326172\t 训练次数：42200, Loss：3.0416259765625\n",
      "430.905641078949\t 训练次数：42300, Loss：2.869342088699341\n",
      "432.05298709869385\t 训练次数：42400, Loss：2.8694143295288086\n",
      "---------- EPOCH 98 ----------\n",
      "\n",
      "433.21525621414185\t 训练次数：42500, Loss：2.8828988075256348\n",
      "434.3763394355774\t 训练次数：42600, Loss：2.881436824798584\n",
      "435.525390625\t 训练次数：42700, Loss：2.872575521469116\n",
      "436.6892294883728\t 训练次数：42800, Loss：2.7372074127197266\n",
      "437.84647369384766\t 训练次数：42900, Loss：2.816742181777954\n",
      "---------- EPOCH 99 ----------\n",
      "439.02662682533264\t 训练次数：43000, Loss：2.8140645027160645\n",
      "440.2096643447876\t 训练次数：43100, Loss：2.7367820739746094\n",
      "441.39471340179443\t 训练次数：43200, Loss：2.8414580821990967\n",
      "442.6044383049011\t 训练次数：43300, Loss：2.85494065284729\n",
      "---------- EPOCH 100 ----------\n",
      "443.7708296775818\t 训练次数：43400, Loss：2.8400042057037354\n",
      "444.95868015289307\t 训练次数：43500, Loss：2.793095350265503\n",
      "446.15450620651245\t 训练次数：43600, Loss：2.7480978965759277\n",
      "447.3343241214752\t 训练次数：43700, Loss：2.913635730743408\n",
      "448.50453543663025\t 训练次数：43800, Loss：2.81312894821167\n",
      "Test Accuracy: 0.9143\n",
      "模型已保存\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# tensorboard 记录训练结果\n",
    "writer = SummaryWriter(\"./logs_train\")\n",
    "\n",
    "# dataloader\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "# 损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, foreach=False)\n",
    "\n",
    "# Training Loop\n",
    "start_time = time.time()\n",
    "total_train_step = 0\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(f\"---------- EPOCH {epoch+1} ----------\\n\")\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # 优化器清除梯度\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # 交叉熵计算损失\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        # 优化器优化模型\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 误差分析\n",
    "        total_train_step += 1\n",
    "        if total_train_step % 100 == 0:\n",
    "            end_time = time.time()\n",
    "            print(f\"{end_time-start_time}\\t 训练次数：{total_train_step}, Loss：{loss}\")\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "            \n",
    "# 测试\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_correct = 0        \n",
    "    total_samples = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        values, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += len(labels)\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    torch.save(model, f\".\\Weights/eeg_gpu.pth\")\n",
    "    # torch.save(light.state_dict(), f\"Weights/light_{epoch}.pth\")\n",
    "    print(\"模型已保存\")\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:28:59.688133200Z",
     "start_time": "2024-02-26T04:21:30.338296900Z"
    }
   },
   "id": "4b19576f5a7dabfd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 测试集\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        values, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += len(labels)\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    torch.save(model, f\".\\Weights/eeg_gpu.pth\")\n",
    "    # torch.save(light.state_dict(), f\"Weights/light_{epoch}.pth\")\n",
    "    print(\"模型已保存\")\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2c56d44a0b8c4e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "57e523c0f932d833"
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 9, 1375])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[-1.5510e+00,  7.4966e+01,  1.0219e+02,  ..., -3.0856e+02,\n",
      "           -3.0519e+02, -4.5943e+02],\n",
      "          [ 6.8855e+00,  4.2774e+01,  1.3900e+01,  ..., -4.0390e+02,\n",
      "           -3.7533e+02, -4.8778e+02],\n",
      "          [-9.6577e+00,  1.2393e+02,  1.1500e+02,  ..., -4.3115e+02,\n",
      "           -4.5605e+02, -5.1908e+02],\n",
      "          ...,\n",
      "          [-1.2002e+02,  3.3730e+01,  3.5003e+01,  ..., -4.0603e+02,\n",
      "           -4.3409e+02, -6.0819e+02],\n",
      "          [-2.2596e+02, -7.0180e+01,  7.1042e+01,  ..., -4.2841e+02,\n",
      "           -3.1199e+02, -6.0364e+02],\n",
      "          [-1.5700e+02, -1.3420e+02,  2.4541e+02,  ..., -6.6344e+02,\n",
      "           -2.2165e+02, -4.9569e+02]]],\n",
      "\n",
      "\n",
      "        [[[-5.0854e+01, -3.0689e+02, -2.5286e+01,  ..., -9.7597e+01,\n",
      "           -1.3549e+01,  1.3606e+02],\n",
      "          [-4.5244e+01, -3.0550e+02, -3.0624e-01,  ..., -1.9304e+02,\n",
      "           -6.7049e+01,  8.9986e+01],\n",
      "          [-7.5914e+01, -3.4179e+02, -1.2330e+01,  ..., -2.1149e+02,\n",
      "           -4.4250e+01,  1.0714e+02],\n",
      "          ...,\n",
      "          [-1.9373e+02, -1.2830e+03, -2.5030e+02,  ...,  1.5443e+03,\n",
      "            9.3067e+02,  6.1053e+02],\n",
      "          [-3.6399e+02, -1.5335e+03, -4.3826e+02,  ...,  5.6381e+02,\n",
      "            9.4409e+01,  1.1902e+02],\n",
      "          [-3.4468e+02, -1.1703e+03, -4.2472e+02,  ..., -7.6970e+02,\n",
      "           -9.8385e+02, -4.0428e+02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0661e+02,  3.0166e+02,  3.3683e+02,  ..., -1.0715e+03,\n",
      "           -2.9486e+02, -2.8668e+02],\n",
      "          [ 7.6787e+01,  2.8646e+02,  3.0390e+02,  ..., -1.0890e+03,\n",
      "           -3.0924e+02, -2.8823e+02],\n",
      "          [ 9.5808e+01,  2.6998e+02,  3.2890e+02,  ..., -1.2175e+03,\n",
      "           -4.0389e+02, -3.1030e+02],\n",
      "          ...,\n",
      "          [ 9.2130e+01,  4.7711e+02,  6.6234e+02,  ..., -2.0953e+03,\n",
      "           -5.9947e+02, -4.8754e+02],\n",
      "          [ 1.2232e+02,  2.6383e+02,  7.9856e+02,  ..., -2.7543e+03,\n",
      "           -1.1804e+03, -7.8707e+02],\n",
      "          [ 1.5442e+02,  2.6301e+02,  6.6249e+02,  ..., -2.4080e+03,\n",
      "           -1.3425e+03, -8.3313e+02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.0174e+02,  2.5696e+02,  1.0859e+01,  ..., -3.7904e+02,\n",
      "           -2.9766e+02, -4.5148e+02],\n",
      "          [ 9.8440e+01,  2.2917e+02, -3.4439e+01,  ..., -3.9858e+02,\n",
      "           -3.1980e+02, -4.4048e+02],\n",
      "          [ 9.4053e+01,  1.6016e+02, -6.4681e+01,  ..., -3.5155e+02,\n",
      "           -2.4295e+02, -4.0313e+02],\n",
      "          ...,\n",
      "          [ 4.5809e+01,  3.3705e+02, -6.0466e+01,  ..., -5.6226e+02,\n",
      "           -7.7627e+02, -8.7231e+02],\n",
      "          [-1.0672e+02, -2.8000e+02, -3.0594e+02,  ..., -7.7972e+02,\n",
      "           -3.7273e+02, -8.6142e+02],\n",
      "          [-1.2919e+02, -6.3528e+02, -6.2298e+02,  ..., -9.3708e+02,\n",
      "           -1.0054e+02, -6.2554e+02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0101e+02, -2.3367e+02,  3.0481e+02,  ..., -8.3347e+02,\n",
      "           -5.1055e+02, -4.6567e+02],\n",
      "          [ 1.7894e+02, -2.6009e+02,  2.6541e+02,  ..., -6.7938e+02,\n",
      "           -4.4872e+02, -4.1623e+02],\n",
      "          [ 2.0814e+02, -1.2619e+02,  3.1601e+02,  ..., -8.4472e+02,\n",
      "           -5.2937e+02, -4.5384e+02],\n",
      "          ...,\n",
      "          [ 2.9336e+02,  3.9420e+02,  8.0450e+02,  ..., -1.3979e+03,\n",
      "           -5.2619e+02, -5.6228e+02],\n",
      "          [ 2.4895e+02,  6.0048e+02,  8.2182e+02,  ..., -2.3008e+03,\n",
      "           -1.5394e+03, -1.0074e+03],\n",
      "          [ 2.5622e+02,  8.3252e+02,  9.8285e+02,  ..., -2.3028e+03,\n",
      "           -1.4352e+03, -9.8297e+02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4645e+02, -1.0400e+03, -7.9579e+01,  ...,  7.0854e+02,\n",
      "            8.3077e+02,  5.8141e+02],\n",
      "          [ 2.5828e+02, -1.0059e+03, -6.6327e+01,  ...,  6.8606e+02,\n",
      "            8.6239e+02,  5.6174e+02],\n",
      "          [ 2.1927e+02, -1.1381e+03, -1.7331e+02,  ...,  8.4569e+02,\n",
      "            9.5295e+02,  6.2592e+02],\n",
      "          ...,\n",
      "          [ 2.2072e+02,  8.9837e+02,  1.4970e+03,  ...,  9.2259e+02,\n",
      "            1.4635e+03,  8.8520e+02],\n",
      "          [ 5.4622e+01,  4.9879e+02,  1.1412e+03,  ...,  2.6311e+03,\n",
      "            2.7265e+03,  1.3473e+03],\n",
      "          [ 1.5844e+00,  4.6006e+02,  9.1619e+02,  ...,  2.8241e+03,\n",
      "            2.6112e+03,  1.3230e+03]]]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(type(inputs))\n",
    "# print(inputs)\n",
    "# inputs = torch.tensor(inputs, requires_grad=True)\n",
    "inputs.requires_grad\n",
    "inputs.retain_grad()\n",
    "limit = max(inputs.min().abs(), inputs.max().abs())\n",
    "print(inputs*limit)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T05:58:47.303381200Z",
     "start_time": "2024-03-01T05:58:47.275873700Z"
    }
   },
   "id": "91ac362022690b1d"
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1012, 1012],\n",
      "         [1039, 1004]],\n",
      "\n",
      "        [[1013, 1000],\n",
      "         [1046, 1046]]]) \n",
      " tensor([[[2, 5],\n",
      "         [2, 9]],\n",
      "\n",
      "        [[9, 5],\n",
      "         [6, 8]]])\n",
      "tensor([[[1010, 1007],\n",
      "         [1037,  995]],\n",
      "\n",
      "        [[1004,  995],\n",
      "         [1040, 1038]]])\n",
      "tensor(5.7500)\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor(np.random.randint(1000, 1050, size=(2, 2, 2))).long()\n",
    "b1 = torch.tensor(np.random.randint(1, 10, size=(2, 2, 2))).long()\n",
    "print(a1, \"\\n\", b1)\n",
    "c1 = a1-b1\n",
    "print(c1)\n",
    "print((c1-a1).float().abs().mean())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T06:15:43.647645600Z",
     "start_time": "2024-03-01T06:15:43.635645900Z"
    }
   },
   "id": "90ad805985c89631"
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32, 33, 34, 35, 36, 37, 38, 39], device='cuda:0', dtype=torch.int32)\n",
      "torch.int64\n",
      "tensor(-54.5292, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(84.2379, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(0.0194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(84.2379, device='cuda:0', grad_fn=<AbsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(inputs)\n",
    "print(labels)\n",
    "print(labels.long().dtype)\n",
    "# 交叉熵计算损失\n",
    "loss = criterion(outputs, labels.long())\n",
    "print(inputs.min())\n",
    "print(inputs.max())\n",
    "print(inputs.mean())\n",
    "print(max(inputs.min().abs(), inputs.max().abs()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T13:38:07.381476100Z",
     "start_time": "2024-02-29T13:38:07.349967300Z"
    }
   },
   "id": "d5946e52139ccfb3"
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [],
   "source": [
    "loss.backward()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T12:12:25.836068600Z",
     "start_time": "2024-02-29T12:12:25.823979400Z"
    }
   },
   "id": "b9fd809cfbfc7f7a"
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([[[[-84.2379, -84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379],\n",
      "          [-84.2379, -84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379],\n",
      "          [ 84.2379,  84.2379,  84.2379,  ...,  84.2379, -84.2379, -84.2379],\n",
      "          ...,\n",
      "          [ 84.2379,  84.2379,  84.2379,  ..., -84.2379, -84.2379, -84.2379],\n",
      "          [ 84.2379,  84.2379,  84.2379,  ..., -84.2379, -84.2379, -84.2379],\n",
      "          [ 84.2379,  84.2379,  84.2379,  ..., -84.2379, -84.2379, -84.2379]]],\n",
      "\n",
      "\n",
      "        [[[ 84.2379,  84.2379,  84.2379,  ..., -84.2379, -84.2379, -84.2379],\n",
      "          [ 84.2379,  84.2379,  84.2379,  ..., -84.2379, -84.2379, -84.2379],\n",
      "          [-84.2379, -84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379],\n",
      "          ...,\n",
      "          [-84.2379, -84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379],\n",
      "          [-84.2379, -84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379],\n",
      "          [-84.2379, -84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379]]],\n",
      "\n",
      "\n",
      "        [[[-84.2379, -84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379],\n",
      "          [-84.2379, -84.2379, -84.2379,  ..., -84.2379, -84.2379,  84.2379],\n",
      "          [ 84.2379,  84.2379,  84.2379,  ..., -84.2379, -84.2379, -84.2379],\n",
      "          ...,\n",
      "          [ 84.2379,  84.2379,  84.2379,  ...,  84.2379,  84.2379,  84.2379],\n",
      "          [-84.2379,  84.2379,  84.2379,  ..., -84.2379, -84.2379, -84.2379],\n",
      "          [ 84.2379,  84.2379,  84.2379,  ..., -84.2379,  84.2379,  84.2379]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-84.2379, -84.2379, -84.2379,  ...,  84.2379, -84.2379, -84.2379],\n",
      "          [ 84.2379, -84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379],\n",
      "          [ 84.2379,  84.2379,  84.2379,  ..., -84.2379,  84.2379,  84.2379],\n",
      "          ...,\n",
      "          [-84.2379, -84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379],\n",
      "          [ 84.2379,  84.2379,  84.2379,  ..., -84.2379,  84.2379,  84.2379],\n",
      "          [-84.2379, -84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379]]],\n",
      "\n",
      "\n",
      "        [[[-84.2379,  84.2379,  84.2379,  ..., -84.2379, -84.2379, -84.2379],\n",
      "          [-84.2379, -84.2379,  84.2379,  ..., -84.2379, -84.2379, -84.2379],\n",
      "          [ 84.2379,  84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379],\n",
      "          ...,\n",
      "          [ 84.2379, -84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379],\n",
      "          [ 84.2379, -84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379],\n",
      "          [-84.2379, -84.2379, -84.2379,  ...,  84.2379,  84.2379,  84.2379]]],\n",
      "\n",
      "\n",
      "        [[[ 84.2404,  84.2384, -84.2394,  ...,  84.2380,  84.2381,  84.2382],\n",
      "          [ 84.2409,  84.2405,  84.2401,  ..., -84.2381, -84.2380, -84.2380],\n",
      "          [-84.2432, -84.2418, -84.2402,  ...,  84.2385,  84.2383,  84.2381],\n",
      "          ...,\n",
      "          [-84.2382, -84.2382, -84.2382,  ..., -84.2381, -84.2380, -84.2379],\n",
      "          [-84.2423, -84.2387,  84.2408,  ..., -84.2380, -84.2382, -84.2385],\n",
      "          [ 84.2389,  84.2387,  84.2383,  ..., -84.2387, -84.2385, -84.2383]]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# inputs.grad\n",
    "print(inputs.is_leaf)\n",
    "# 梯度清零\n",
    "# print(inputs.grad.fill_(0))\n",
    "# 梯度取符号\n",
    "print(inputs.grad*torch.max(inputs))\n",
    "# 返回梯度的符号，而不改变梯度值\n",
    "# print(inputs.grad.sign())\n",
    "# print(inputs.grad)\n",
    "# 改变梯度值\n",
    "# print(inputs.grad.sign_())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T12:12:47.057901300Z",
     "start_time": "2024-02-29T12:12:47.026403300Z"
    }
   },
   "id": "cf2566251921c615"
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "ll = inputs.detach().cpu().numpy()\n",
    "print((inputs.detach().cpu().numpy() == inputs.detach().cpu().numpy()).sum()/len(ll.flatten()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T13:05:37.679374900Z",
     "start_time": "2024-02-29T13:05:37.666375700Z"
    }
   },
   "id": "9334c93e0d19bc07"
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([153.], dtype=torch.float64, grad_fn=<SqueezeBackward4>)\n",
      "None\n",
      "None\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def mat(x, y):\n",
    "    return x@y\n",
    "\n",
    "aa = torch.tensor([12, 32, 11], dtype=torch.float64, requires_grad=True)\n",
    "bb = torch.randint(2, 5, size=(3, 1), dtype=torch.float64, requires_grad=True)\n",
    "cc = aa.clone().detach()\n",
    "cc.requires_grad=True\n",
    "dd = mat(aa, bb)\n",
    "print(dd)\n",
    "dd.backward()\n",
    "\n",
    "aa = aa.clone().detach()\n",
    "print(aa.grad)\n",
    "print(cc.grad)\n",
    "print(aa.is_leaf)\n",
    "print(cc.is_leaf)\n",
    "print(cc.requires_grad)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:18:58.511632200Z",
     "start_time": "2024-03-02T12:18:58.495620500Z"
    }
   },
   "id": "f602e68ea20398e"
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32, 23, 47, 13, 18], dtype=torch.int32)\n",
      "(5, 9, 1375)\n",
      "haha\n",
      "tensor([1, 1, 1, 1, 1])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "mm = torch.tensor(np.random.randint(0, 50, size=5))\n",
    "print(mm)\n",
    "print(train_loader.dataset.data[mm].shape)\n",
    "print(\"haha\")\n",
    "print(torch.LongTensor(mm.size()).fill_(1))\n",
    "print(mm.size())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T07:03:24.512408900Z",
     "start_time": "2024-02-29T07:03:24.495766900Z"
    }
   },
   "id": "4c916b19155f4de5"
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([[1, 2], [2, 3]]).dim())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:12:33.495503200Z",
     "start_time": "2024-02-29T14:12:33.486057600Z"
    }
   },
   "id": "b6f84da8ee023bba"
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 2])\n",
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "qjs = np.random.randint(1, 5, size=(1, 2, 3))\n",
    "trans=transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "    ])\n",
    "xx = trans(qjs)\n",
    "print(xx.shape)\n",
    "print(xx.permute(1, 2, 0).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:20:02.401369200Z",
     "start_time": "2024-02-29T14:20:02.386369600Z"
    }
   },
   "id": "8841b54738ec7e1e"
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0184,  0.8899,  1.2132,  ..., -3.6629, -3.6229, -5.4540],\n",
      "         [ 0.0817,  0.5078,  0.1650,  ..., -4.7947, -4.4556, -5.7905],\n",
      "         [-0.1146,  1.4712,  1.3652,  ..., -5.1182, -5.4138, -6.1621],\n",
      "         ...,\n",
      "         [-1.4247,  0.4004,  0.4155,  ..., -4.8200, -5.1531, -7.2199],\n",
      "         [-2.6824, -0.8331,  0.8434,  ..., -5.0857, -3.7036, -7.1658],\n",
      "         [-1.8637, -1.5931,  2.9133,  ..., -7.8758, -2.6312, -5.8844]]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([[[[-2.1858e-04,  1.0564e-02,  1.4402e-02,  ..., -4.3483e-02,\n",
      "           -4.3008e-02, -6.4745e-02],\n",
      "          [ 9.7033e-04,  6.0279e-03,  1.9588e-03,  ..., -5.6919e-02,\n",
      "           -5.2893e-02, -6.8740e-02],\n",
      "          [-1.3610e-03,  1.7465e-02,  1.6207e-02,  ..., -6.0759e-02,\n",
      "           -6.4268e-02, -7.3151e-02],\n",
      "          ...,\n",
      "          [-1.6913e-02,  4.7534e-03,  4.9328e-03,  ..., -5.7219e-02,\n",
      "           -6.1174e-02, -8.5708e-02],\n",
      "          [-3.1844e-02, -9.8900e-03,  1.0012e-02,  ..., -6.0373e-02,\n",
      "           -4.3966e-02, -8.5067e-02],\n",
      "          [-2.2125e-02, -1.8912e-02,  3.4585e-02,  ..., -9.3494e-02,\n",
      "           -3.1235e-02, -6.9854e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.1665e-03, -4.3248e-02, -3.5634e-03,  ..., -1.3754e-02,\n",
      "           -1.9094e-03,  1.9175e-02],\n",
      "          [-6.3760e-03, -4.3052e-02, -4.3157e-05,  ..., -2.7204e-02,\n",
      "           -9.4488e-03,  1.2681e-02],\n",
      "          [-1.0698e-02, -4.8167e-02, -1.7376e-03,  ..., -2.9804e-02,\n",
      "           -6.2359e-03,  1.5099e-02],\n",
      "          ...,\n",
      "          [-2.7301e-02, -1.8081e-01, -3.5273e-02,  ...,  2.1763e-01,\n",
      "            1.3115e-01,  8.6038e-02],\n",
      "          [-5.1295e-02, -2.1611e-01, -6.1761e-02,  ...,  7.9454e-02,\n",
      "            1.3304e-02,  1.6772e-02],\n",
      "          [-4.8574e-02, -1.6492e-01, -5.9853e-02,  ..., -1.0847e-01,\n",
      "           -1.3865e-01, -5.6972e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5024e-02,  4.2511e-02,  4.7467e-02,  ..., -1.5100e-01,\n",
      "           -4.1553e-02, -4.0399e-02],\n",
      "          [ 1.0821e-02,  4.0369e-02,  4.2827e-02,  ..., -1.5346e-01,\n",
      "           -4.3580e-02, -4.0619e-02],\n",
      "          [ 1.3502e-02,  3.8047e-02,  4.6350e-02,  ..., -1.7157e-01,\n",
      "           -5.6918e-02, -4.3729e-02],\n",
      "          ...,\n",
      "          [ 1.2983e-02,  6.7236e-02,  9.3340e-02,  ..., -2.9527e-01,\n",
      "           -8.4480e-02, -6.8706e-02],\n",
      "          [ 1.7238e-02,  3.7181e-02,  1.1254e-01,  ..., -3.8815e-01,\n",
      "           -1.6634e-01, -1.1092e-01],\n",
      "          [ 2.1761e-02,  3.7064e-02,  9.3360e-02,  ..., -3.3934e-01,\n",
      "           -1.8919e-01, -1.1741e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.4338e-02,  3.6212e-02,  1.5303e-03,  ..., -5.3416e-02,\n",
      "           -4.1947e-02, -6.3625e-02],\n",
      "          [ 1.3872e-02,  3.2296e-02, -4.8532e-03,  ..., -5.6169e-02,\n",
      "           -4.5067e-02, -6.2074e-02],\n",
      "          [ 1.3254e-02,  2.2571e-02, -9.1151e-03,  ..., -4.9542e-02,\n",
      "           -3.4237e-02, -5.6811e-02],\n",
      "          ...,\n",
      "          [ 6.4556e-03,  4.7498e-02, -8.5210e-03,  ..., -7.9236e-02,\n",
      "           -1.0940e-01, -1.2293e-01],\n",
      "          [-1.5039e-02, -3.9459e-02, -4.3115e-02,  ..., -1.0988e-01,\n",
      "           -5.2527e-02, -1.2140e-01],\n",
      "          [-1.8206e-02, -8.9526e-02, -8.7793e-02,  ..., -1.3206e-01,\n",
      "           -1.4168e-02, -8.8153e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.8327e-02, -3.2930e-02,  4.2955e-02,  ..., -1.1746e-01,\n",
      "           -7.1949e-02, -6.5624e-02],\n",
      "          [ 2.5217e-02, -3.6652e-02,  3.7402e-02,  ..., -9.5741e-02,\n",
      "           -6.3235e-02, -5.8657e-02],\n",
      "          [ 2.9332e-02, -1.7783e-02,  4.4534e-02,  ..., -1.1904e-01,\n",
      "           -7.4600e-02, -6.3957e-02],\n",
      "          ...,\n",
      "          [ 4.1341e-02,  5.5552e-02,  1.1337e-01,  ..., -1.9700e-01,\n",
      "           -7.4153e-02, -7.9239e-02],\n",
      "          [ 3.5083e-02,  8.4623e-02,  1.1581e-01,  ..., -3.2424e-01,\n",
      "           -2.1693e-01, -1.4197e-01],\n",
      "          [ 3.6108e-02,  1.1732e-01,  1.3851e-01,  ..., -3.2452e-01,\n",
      "           -2.0225e-01, -1.3852e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4731e-02, -1.4656e-01, -1.1215e-02,  ...,  9.9850e-02,\n",
      "            1.1708e-01,  8.1935e-02],\n",
      "          [ 3.6398e-02, -1.4176e-01, -9.3471e-03,  ...,  9.6683e-02,\n",
      "            1.2153e-01,  7.9163e-02],\n",
      "          [ 3.0901e-02, -1.6039e-01, -2.4423e-02,  ...,  1.1918e-01,\n",
      "            1.3429e-01,  8.8207e-02],\n",
      "          ...,\n",
      "          [ 3.1104e-02,  1.2660e-01,  2.1096e-01,  ...,  1.3001e-01,\n",
      "            2.0625e-01,  1.2475e-01],\n",
      "          [ 7.6976e-03,  7.0291e-02,  1.6082e-01,  ...,  3.7079e-01,\n",
      "            3.8423e-01,  1.8987e-01],\n",
      "          [ 2.2328e-04,  6.4833e-02,  1.2911e-01,  ...,  3.9798e-01,\n",
      "            3.6799e-01,  1.8644e-01]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "[[[[-2.18578018e-04  1.05644995e-02  1.44016733e-02 ... -4.34833318e-02\n",
      "    -4.30080481e-02 -6.47453442e-02]\n",
      "   [ 9.70331836e-04  6.02789270e-03  1.95879559e-03 ... -5.69187216e-02\n",
      "    -5.28932661e-02 -6.87399656e-02]\n",
      "   [-1.36100000e-03  1.74649116e-02  1.62067022e-02 ... -6.07593656e-02\n",
      "    -6.42678440e-02 -7.31509477e-02]\n",
      "   ...\n",
      "   [-1.69130005e-02  4.75342618e-03  4.93279472e-03 ... -5.72192371e-02\n",
      "    -6.11736141e-02 -8.57079029e-02]\n",
      "   [-3.18436399e-02 -9.89002455e-03  1.00115817e-02 ... -6.03732839e-02\n",
      "    -4.39663641e-02 -8.50666314e-02]\n",
      "   [-2.21245922e-02 -1.89115871e-02  3.45846899e-02 ... -9.34944078e-02\n",
      "    -3.12352125e-02 -6.98542669e-02]]]\n",
      "\n",
      "\n",
      " [[[-7.16654630e-03 -4.32481505e-02 -3.56343132e-03 ... -1.37537587e-02\n",
      "    -1.90937670e-03  1.91746242e-02]\n",
      "   [-6.37599314e-03 -4.30519506e-02 -4.31572480e-05 ... -2.72041559e-02\n",
      "    -9.44881514e-03  1.26811620e-02]\n",
      "   [-1.06981397e-02 -4.81669530e-02 -1.73763582e-03 ... -2.98038106e-02\n",
      "    -6.23594271e-03  1.50992023e-02]\n",
      "   ...\n",
      "   [-2.73014084e-02 -1.80808902e-01 -3.52727138e-02 ...  2.17628837e-01\n",
      "     1.31153509e-01  8.60378742e-02]\n",
      "   [-5.12947217e-02 -2.16105655e-01 -6.17612675e-02 ...  7.94543549e-02\n",
      "     1.33044776e-02  1.67722218e-02]\n",
      "   [-4.85736653e-02 -1.64918512e-01 -5.98531887e-02 ... -1.08469598e-01\n",
      "    -1.38647497e-01 -5.69720455e-02]]]\n",
      "\n",
      "\n",
      " [[[ 1.50236618e-02  4.25112098e-02  4.74671759e-02 ... -1.50999844e-01\n",
      "    -4.15534042e-02 -4.03994434e-02]\n",
      "   [ 1.08210687e-02  4.03685123e-02  4.28271666e-02 ... -1.53463647e-01\n",
      "    -4.35795039e-02 -4.06189971e-02]\n",
      "   [ 1.35016507e-02  3.80468853e-02  4.63500805e-02 ... -1.71574205e-01\n",
      "    -5.69177493e-02 -4.37287837e-02]\n",
      "   ...\n",
      "   [ 1.29832868e-02  6.72359094e-02  9.33398604e-02 ... -2.95274705e-01\n",
      "    -8.44798833e-02 -6.87061250e-02]\n",
      "   [ 1.72381271e-02  3.71806137e-02  1.12536885e-01 ... -3.88148993e-01\n",
      "    -1.66340753e-01 -1.10916860e-01]\n",
      "   [ 2.17608064e-02  3.70642617e-02  9.33603346e-02 ... -3.39338899e-01\n",
      "    -1.89194813e-01 -1.17407553e-01]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 1.43379550e-02  3.62117998e-02  1.53027556e-03 ... -5.34157567e-02\n",
      "    -4.19468842e-02 -6.36249632e-02]\n",
      "   [ 1.38724977e-02  3.22959088e-02 -4.85324068e-03 ... -5.61690815e-02\n",
      "    -4.50670198e-02 -6.20736144e-02]\n",
      "   [ 1.32542625e-02  2.25706324e-02 -9.11508035e-03 ... -4.95423749e-02\n",
      "    -3.42369713e-02 -5.68106100e-02]\n",
      "   ...\n",
      "   [ 6.45560585e-03  4.74981144e-02 -8.52104276e-03 ... -7.92363882e-02\n",
      "    -1.09395228e-01 -1.22930035e-01]\n",
      "   [-1.50390007e-02 -3.94585729e-02 -4.31145318e-02 ... -1.09881341e-01\n",
      "    -5.25268614e-02 -1.21395148e-01]\n",
      "   [-1.82058625e-02 -8.95259604e-02 -8.77925754e-02 ... -1.32057339e-01\n",
      "    -1.41684664e-02 -8.81528631e-02]]]\n",
      "\n",
      "\n",
      " [[[ 2.83265579e-02 -3.29303928e-02  4.29554544e-02 ... -1.17456585e-01\n",
      "    -7.19490051e-02 -6.56239241e-02]\n",
      "   [ 2.52174754e-02 -3.66522856e-02  3.74024548e-02 ... -9.57408026e-02\n",
      "    -6.32350072e-02 -5.86566739e-02]\n",
      "   [ 2.93324906e-02 -1.77832674e-02  4.45336960e-02 ... -1.19041696e-01\n",
      "    -7.46002942e-02 -6.39568940e-02]\n",
      "   ...\n",
      "   [ 4.13407646e-02  5.55519909e-02  1.13372937e-01 ... -1.96995050e-01\n",
      "    -7.41531551e-02 -7.92391151e-02]\n",
      "   [ 3.50831673e-02  8.46225619e-02  1.15814321e-01 ... -3.24239373e-01\n",
      "    -2.16932654e-01 -1.41967520e-01]\n",
      "   [ 3.61080728e-02  1.17321976e-01  1.38507634e-01 ... -3.24518621e-01\n",
      "    -2.02252358e-01 -1.38524607e-01]]]\n",
      "\n",
      "\n",
      " [[[ 3.47312540e-02 -1.46555275e-01 -1.12145394e-02 ...  9.98503864e-02\n",
      "     1.17075093e-01  8.19350630e-02]\n",
      "   [ 3.63979153e-02 -1.41760975e-01 -9.34712868e-03 ...  9.66828391e-02\n",
      "     1.21531822e-01  7.91626051e-02]\n",
      "   [ 3.09005324e-02 -1.60388499e-01 -2.44233068e-02 ...  1.19177528e-01\n",
      "     1.34294122e-01  8.82071555e-02]\n",
      "   ...\n",
      "   [ 3.11041549e-02  1.26602530e-01  2.10957453e-01 ...  1.30014822e-01\n",
      "     2.06246316e-01  1.24746419e-01]\n",
      "   [ 7.69761018e-03  7.02911168e-02  1.60822108e-01 ...  3.70791107e-01\n",
      "     3.84230912e-01  1.89867809e-01]\n",
      "   [ 2.23278126e-04  6.48328885e-02  1.29112899e-01 ...  3.97979379e-01\n",
      "     3.67985517e-01  1.86443239e-01]]]]\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])\n",
    "b = inputs/inputs.max()\n",
    "print(b)\n",
    "print(b.detach().cpu().numpy())\n",
    "print(b.device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T05:27:41.611681Z",
     "start_time": "2024-03-01T05:27:41.581681200Z"
    }
   },
   "id": "6ca692843aa21def"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# SummaryWriter测试\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"./log_test\", filename_suffix=\"qjs\")\n",
    "for i in range(10):\n",
    "    writer.add_scalar(\"qjs/x**3\",scalar_value=i**3, global_step=i)\n",
    "    writer.add_scalar(\"qjs/x*3\", scalar_value=i*3, global_step=i)\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T08:55:17.034287400Z",
     "start_time": "2024-02-27T08:55:17.016281800Z"
    }
   },
   "id": "b56da2a0a2baa4a5"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2066, -0.5919, -0.3954], grad_fn=<SubBackward0>)\n",
      "tensor([0.0427, 0.3503, 0.1564], grad_fn=<PowBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hello\\AppData\\Local\\Temp\\ipykernel_28644\\881887349.py:9: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  x.grad\n",
      "C:\\Users\\hello\\AppData\\Local\\Temp\\ipykernel_28644\\881887349.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  x.grad.sign()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sign'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[78], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m y\u001B[38;5;241m.\u001B[39mbackward(torch\u001B[38;5;241m.\u001B[39mones_like(x))\n\u001B[0;32m      9\u001B[0m x\u001B[38;5;241m.\u001B[39mgrad\n\u001B[1;32m---> 10\u001B[0m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msign\u001B[49m()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'sign'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 自动求导测试\n",
    "def func1(x):\n",
    "    return x**2\n",
    "x = np.rand(size=(3, ), requires_grad=True)\n",
    "print(x)\n",
    "y = func1(x)\n",
    "print(y)\n",
    "y.backward(torch.ones_like(x))\n",
    "x.grad\n",
    "x.grad.sign()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:27:42.449827600Z",
     "start_time": "2024-02-27T13:27:42.393930200Z"
    }
   },
   "id": "5a7e143b42e9a015"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([6, 0])"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[1, 2, 3, 4, 5, 6, 7], [8, 5, 2, 5, 4, 6 ,1]])\n",
    "b = torch.Tensor([1, 3, 2, 5, 4, 6, 7])\n",
    "print(torch.eq(a, b).float().mean().data)\n",
    "torch.argmax(a, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T13:37:48.346080800Z",
     "start_time": "2024-02-28T13:37:48.337666400Z"
    }
   },
   "id": "99be5b14a1f954f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "Python Torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
