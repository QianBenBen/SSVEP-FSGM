{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# EEG SSVEP Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d726ba570e097eb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 一、Dataset类创建"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3b19d8623132aa4"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:20:02.293160100Z",
     "start_time": "2024-02-26T04:20:02.275053900Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Benchmark(Dataset):\n",
    "    \n",
    "    classes = {\n",
    "        \n",
    "    }\n",
    "    \n",
    "    stim_event_freq = [8., 8.2, 8.4, 8.6, 8.8, 9., 9.2, 9.4, 9.6, 9.8, 10., 10.2, 10.4, 10.6,\n",
    "                       10.8, 11., 11.2, 11.4, 11.6, 11.8, 12., 12.2, 12.4, 12.6, 12.8, 13., 13.2, 13.4,\n",
    "                       13.6, 13.8, 14., 14.2, 14.4, 14.6, 14.8, 15., 15.2, 15.4, 15.6, 15.8]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str = '',\n",
    "        train: bool = True,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super(Dataset).__init__()\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.subject_num = 35\n",
    "        # 采样率1000，降采样至 250Hz\n",
    "        self.samp_rate = 250\n",
    "        # 预处理滤波器设置\n",
    "        '''没看懂'''\n",
    "        self.filterB, self.filterA = self.__get_pre_filter(self.samp_rate)\n",
    "        self.data, self.pre_data, self.label = self.load_data()\n",
    "    \n",
    "    def load_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        channels = [53, 54, 55, 57, 58, 59, 61, 62, 63]\n",
    "        channels = [i - 1 for i in channels]\n",
    "        \n",
    "        if self.train:\n",
    "            # train data\n",
    "            print(\"---------- 训练数据加载 ----------\")            \n",
    "            data = np.zeros((200*self.subject_num, len(channels), 1375))\n",
    "            pre_data = np.zeros((200*self.subject_num, len(channels), 125))\n",
    "            label = np.zeros(200*self.subject_num, dtype=int)\n",
    "        else:\n",
    "            # test data\n",
    "            print(\"---------- 测试数据加载 ----------\")\n",
    "            data = np.zeros((40*self.subject_num, len(channels), 1375))\n",
    "            pre_data = np.zeros((40*self.subject_num, len(channels), 125))\n",
    "            label = np.zeros(40*self.subject_num, dtype=int)\n",
    "            \n",
    "        for sub_num in tqdm(range(1, self.subject_num+1)):\n",
    "            f = scipy.io.loadmat(self.root + f\"/S{sub_num}.mat\")\n",
    "            # print(f\"mat{sub_num}文件大小: {f['data'].shape}\")\n",
    "            for block in range(6):\n",
    "                for target in range(40):\n",
    "                    if self.train and block!=5:\n",
    "                        data[(sub_num - 1) * 200 + block * 40 + target] = f[\"data\"][channels, 125:, target, block]\n",
    "                        pre_data[(sub_num - 1) * 200 + block * 40 + target] = f[\"data\"][channels, :125, target, block]\n",
    "                        label[(sub_num - 1) * 200 + block * 40 + target] = int(target)\n",
    "                    elif not self.train and block==5:\n",
    "                        data[(sub_num - 1) * 40 + target] = f[\"data\"][channels, 125:, target, block]\n",
    "                        pre_data[(sub_num - 1) * 40 + target] = f[\"data\"][channels, :125, target, block]\n",
    "                        label[(sub_num - 1) * 40 + target] = int(target)\n",
    "        return data, pre_data, label\n",
    "    \n",
    "    def __get_pre_filter(self, samp_rate):\n",
    "        fs = samp_rate\n",
    "        f0 = 50\n",
    "        q = 35\n",
    "        b, a = signal.iircomb(f0, q, ftype='notch', fs=fs)\n",
    "        return b, a\n",
    "    \n",
    "    def __preprocess(self, data):\n",
    "        filter_data = signal.filtfilt(self.filterB, self.filterA, data)\n",
    "        return filter_data\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index) -> Tuple[Any, Any]:\n",
    "        eeg, target = self.data[index], self.label[index]\n",
    "        \n",
    "        # 滤波处理\n",
    "        eeg = self.__preprocess(eeg)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            eeg = self.transform(eeg.copy())\n",
    "            \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target.copy())\n",
    "        \n",
    "        eeg = eeg.float()\n",
    "        return eeg, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 训练数据加载 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:19<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 测试数据加载 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:19<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# torch.set_default_dtype(torch.float64)\n",
    "train_data = Benchmark(\"E:\\Datasets\\BCI\\SSVEP\\Benchmark\", train = True, \n",
    "                       transform = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ]))\n",
    "test_data = Benchmark(\"E:\\Datasets\\BCI\\SSVEP\\Benchmark\", train = False, \n",
    "                       transform = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:20:42.352178900Z",
     "start_time": "2024-02-26T04:20:03.387314500Z"
    }
   },
   "id": "b2e928e06e622fd7"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:\n",
      " 7000\n",
      "shape: torch.Size([1, 9, 1375]), type: <class 'torch.Tensor'>\n",
      "test_data:\n",
      " 1400\n",
      "shape: torch.Size([1, 9, 1375]), type: <class 'torch.Tensor'>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data:\\n\", len(train_data))\n",
    "print(f\"shape: {train_data[0][0].shape}, type: {type(train_data[0][0])}\")\n",
    "\n",
    "print(\"test_data:\\n\", len(test_data))\n",
    "print(f\"shape: {test_data[0][0].shape}, type: {type(test_data[0][0])}\")\n",
    "\n",
    "print(train_data[2][1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:20:47.316825200Z",
     "start_time": "2024-02-26T04:20:47.297820100Z"
    }
   },
   "id": "b5a91edee5bf87c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 二、EEGNet构建"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74cc6cfc51129a49"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 8, 9, 1376]             512\n",
      "       BatchNorm2d-2           [-1, 8, 9, 1376]              16\n",
      "            Conv2d-3          [-1, 16, 1, 1376]             144\n",
      "            Conv2d-4           [-1, 8, 1, 1376]             128\n",
      "DepthwiseSeparableConv2d-5           [-1, 8, 1, 1376]               0\n",
      "       BatchNorm2d-6           [-1, 8, 1, 1376]              16\n",
      "               ELU-7           [-1, 8, 1, 1376]               0\n",
      "         AvgPool2d-8            [-1, 8, 1, 344]               0\n",
      "           Dropout-9            [-1, 8, 1, 344]               0\n",
      "           Conv2d-10            [-1, 8, 1, 345]             128\n",
      "           Conv2d-11           [-1, 16, 1, 345]             128\n",
      "DepthwiseSeparableConv2d-12           [-1, 16, 1, 345]               0\n",
      "      BatchNorm2d-13           [-1, 16, 1, 345]              32\n",
      "              ELU-14           [-1, 16, 1, 345]               0\n",
      "        AvgPool2d-15            [-1, 16, 1, 43]               0\n",
      "          Dropout-16            [-1, 16, 1, 43]               0\n",
      "          Flatten-17                  [-1, 688]               0\n",
      "           Linear-18                   [-1, 40]          27,560\n",
      "          Softmax-19                   [-1, 40]               0\n",
      "================================================================\n",
      "Total params: 28,664\n",
      "Trainable params: 28,664\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 2.26\n",
      "Params size (MB): 0.11\n",
      "Estimated Total Size (MB): 2.42\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, depth_multiplier=1):\n",
    "        super(DepthwiseSeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels * depth_multiplier, kernel_size=kernel_size,\n",
    "                                   stride=(1, 1), padding=(0, 0 if kernel_size[0]>kernel_size[1] else max(kernel_size)//2), groups=in_channels, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_channels * depth_multiplier, out_channels, kernel_size=(1, 1),\n",
    "                                   stride=(1, 1), padding=(0, 0), bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes, Chans=64, Samples=128, dropoutRate=0.5, kernLength=64,\n",
    "                 F1=8, D=2, F2=16, norm_rate=0.25, dropoutType='Dropout'):\n",
    "        super(EEGNet, self).__init__()\n",
    "\n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            self.dropoutType = nn.Dropout2d\n",
    "        elif dropoutType == 'Dropout':\n",
    "            self.dropoutType = nn.Dropout\n",
    "        else:\n",
    "            raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
    "                             'or Dropout, passed as a string.')\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, F1, (1, kernLength), padding=(0, kernLength//2), bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            DepthwiseSeparableConv2d(F1, F1, kernel_size=(Chans, 1), depth_multiplier=D),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            self.dropoutType(dropoutRate)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(F1, F2, kernel_size=(1, 16), depth_multiplier=1),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8)),\n",
    "            self.dropoutType(dropoutRate)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(F2*int(np.floor((np.floor((Samples+1)/4)+1)/8)), nb_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.block1(input)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        return x\n",
    "\n",
    "learning_rate = 1e-3\n",
    "nb_classes = 40\n",
    "Chans = 9\n",
    "Samples = 1375\n",
    "model = EEGNet(nb_classes, Chans, Samples)\n",
    "model = model.to(device)\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "# print(model.state_dict()['block1.0.weight'].dtype)\n",
    "# print(device)\n",
    "print(summary(model, input_size=(1, 9, 1375), device='cuda'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:20:52.273670700Z",
     "start_time": "2024-02-26T04:20:52.012828100Z"
    }
   },
   "id": "9b344932ffcc584d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- EPOCH 1 ----------\n",
      "0.8241136074066162\t 训练次数：100, Loss：3.69568133354187\n",
      "1.420565128326416\t 训练次数：200, Loss：3.6918246746063232\n",
      "2.025568723678589\t 训练次数：300, Loss：3.6824088096618652\n",
      "2.6121058464050293\t 训练次数：400, Loss：3.658505439758301\n",
      "---------- EPOCH 2 ----------\n",
      "3.2184486389160156\t 训练次数：500, Loss：3.3628623485565186\n",
      "3.818922281265259\t 训练次数：600, Loss：3.4165542125701904\n",
      "4.409793376922607\t 训练次数：700, Loss：3.315293312072754\n",
      "4.999252080917358\t 训练次数：800, Loss：3.1848251819610596\n",
      "---------- EPOCH 3 ----------\n",
      "\n",
      "5.563786268234253\t 训练次数：900, Loss：3.342639207839966\n",
      "6.145760536193848\t 训练次数：1000, Loss：3.3122804164886475\n",
      "6.715238571166992\t 训练次数：1100, Loss：3.366192579269409\n",
      "7.293615102767944\t 训练次数：1200, Loss：3.1675117015838623\n",
      "7.869288444519043\t 训练次数：1300, Loss：3.445911407470703\n",
      "---------- EPOCH 4 ----------\n",
      "8.412700414657593\t 训练次数：1400, Loss：3.4322941303253174\n",
      "8.95007061958313\t 训练次数：1500, Loss：3.1146395206451416\n",
      "9.499636888504028\t 训练次数：1600, Loss：3.17037034034729\n",
      "10.048905849456787\t 训练次数：1700, Loss：3.2558743953704834\n",
      "---------- EPOCH 5 ----------\n",
      "10.59870457649231\t 训练次数：1800, Loss：3.102684736251831\n",
      "11.151061296463013\t 训练次数：1900, Loss：3.0119166374206543\n",
      "11.699729919433594\t 训练次数：2000, Loss：3.3211851119995117\n",
      "12.241151094436646\t 训练次数：2100, Loss：3.0903100967407227\n",
      "---------- EPOCH 6 ----------\n",
      "\n",
      "12.7729332447052\t 训练次数：2200, Loss：3.055176019668579\n",
      "13.314518451690674\t 训练次数：2300, Loss：2.9931983947753906\n",
      "13.873722553253174\t 训练次数：2400, Loss：3.247196674346924\n",
      "14.43396544456482\t 训练次数：2500, Loss：2.9720544815063477\n",
      "15.007543325424194\t 训练次数：2600, Loss：3.0846807956695557\n",
      "---------- EPOCH 7 ----------\n",
      "15.570297241210938\t 训练次数：2700, Loss：3.2866628170013428\n",
      "16.138839960098267\t 训练次数：2800, Loss：3.1078319549560547\n",
      "16.70604681968689\t 训练次数：2900, Loss：3.0503664016723633\n",
      "17.265702486038208\t 训练次数：3000, Loss：3.1067521572113037\n",
      "---------- EPOCH 8 ----------\n",
      "\n",
      "17.84904170036316\t 训练次数：3100, Loss：3.1734471321105957\n",
      "18.432990550994873\t 训练次数：3200, Loss：3.0866775512695312\n",
      "19.00852084159851\t 训练次数：3300, Loss：2.9757895469665527\n",
      "19.576961040496826\t 训练次数：3400, Loss：3.1468448638916016\n",
      "20.149229049682617\t 训练次数：3500, Loss：2.994126319885254\n",
      "---------- EPOCH 9 ----------\n",
      "20.73979878425598\t 训练次数：3600, Loss：3.071685791015625\n",
      "21.313004970550537\t 训练次数：3700, Loss：2.9142415523529053\n",
      "21.893361806869507\t 训练次数：3800, Loss：3.057769298553467\n",
      "22.47278904914856\t 训练次数：3900, Loss：3.142874002456665\n",
      "---------- EPOCH 10 ----------\n",
      "23.050809860229492\t 训练次数：4000, Loss：2.9856269359588623\n",
      "23.624848127365112\t 训练次数：4100, Loss：3.0594727993011475\n",
      "24.215673208236694\t 训练次数：4200, Loss：3.117633581161499\n",
      "24.80285382270813\t 训练次数：4300, Loss：2.8218610286712646\n",
      "---------- EPOCH 11 ----------\n",
      "\n",
      "25.371490240097046\t 训练次数：4400, Loss：2.998093366622925\n",
      "25.92788290977478\t 训练次数：4500, Loss：2.995107889175415\n",
      "26.481305599212646\t 训练次数：4600, Loss：3.1153838634490967\n",
      "27.030988931655884\t 训练次数：4700, Loss：2.9327008724212646\n",
      "27.592524528503418\t 训练次数：4800, Loss：2.9374966621398926\n",
      "---------- EPOCH 12 ----------\n",
      "28.138570070266724\t 训练次数：4900, Loss：2.8450796604156494\n",
      "28.7111656665802\t 训练次数：5000, Loss：3.1328203678131104\n",
      "29.267690658569336\t 训练次数：5100, Loss：3.048942804336548\n",
      "29.814852476119995\t 训练次数：5200, Loss：3.141753673553467\n",
      "---------- EPOCH 13 ----------\n",
      "30.35434055328369\t 训练次数：5300, Loss：3.0191848278045654\n",
      "30.88556432723999\t 训练次数：5400, Loss：2.973623514175415\n",
      "31.420557737350464\t 训练次数：5500, Loss：3.130657911300659\n",
      "31.969501972198486\t 训练次数：5600, Loss：3.258699893951416\n",
      "---------- EPOCH 14 ----------\n",
      "\n",
      "32.50787305831909\t 训练次数：5700, Loss：3.0611119270324707\n",
      "33.04680156707764\t 训练次数：5800, Loss：2.9511826038360596\n",
      "33.582480669021606\t 训练次数：5900, Loss：2.8817081451416016\n",
      "34.13181734085083\t 训练次数：6000, Loss：3.0659189224243164\n",
      "34.70740246772766\t 训练次数：6100, Loss：3.0912909507751465\n",
      "---------- EPOCH 15 ----------\n",
      "35.27678823471069\t 训练次数：6200, Loss：2.870492935180664\n",
      "35.83446788787842\t 训练次数：6300, Loss：2.9434525966644287\n",
      "36.390058517456055\t 训练次数：6400, Loss：2.898453712463379\n",
      "36.93854808807373\t 训练次数：6500, Loss：2.9282240867614746\n",
      "---------- EPOCH 16 ----------\n",
      "\n",
      "37.47760343551636\t 训练次数：6600, Loss：2.9336330890655518\n",
      "38.027236461639404\t 训练次数：6700, Loss：3.1304919719696045\n",
      "38.58481431007385\t 训练次数：6800, Loss：3.0128543376922607\n",
      "39.13854455947876\t 训练次数：6900, Loss：2.8728442192077637\n",
      "39.68108916282654\t 训练次数：7000, Loss：3.0574941635131836\n",
      "---------- EPOCH 17 ----------\n",
      "40.22420287132263\t 训练次数：7100, Loss：3.091304302215576\n",
      "40.78425693511963\t 训练次数：7200, Loss：3.0364503860473633\n",
      "41.35036015510559\t 训练次数：7300, Loss：2.9823029041290283\n",
      "41.925028800964355\t 训练次数：7400, Loss：2.954481601715088\n",
      "---------- EPOCH 18 ----------\n",
      "42.51222467422485\t 训练次数：7500, Loss：2.919877767562866\n",
      "43.1111056804657\t 训练次数：7600, Loss：2.9326417446136475\n",
      "43.6729097366333\t 训练次数：7700, Loss：2.902134895324707\n",
      "44.236443519592285\t 训练次数：7800, Loss：3.0319340229034424\n",
      "---------- EPOCH 19 ----------\n",
      "\n",
      "44.81301140785217\t 训练次数：7900, Loss：2.8612098693847656\n",
      "45.388543128967285\t 训练次数：8000, Loss：3.0847442150115967\n",
      "45.958746671676636\t 训练次数：8100, Loss：3.043628454208374\n",
      "46.53875756263733\t 训练次数：8200, Loss：3.0214691162109375\n",
      "47.11708211898804\t 训练次数：8300, Loss：2.910719156265259\n",
      "---------- EPOCH 20 ----------\n",
      "47.701716899871826\t 训练次数：8400, Loss：2.915083646774292\n",
      "48.28000020980835\t 训练次数：8500, Loss：2.8846962451934814\n",
      "48.855254888534546\t 训练次数：8600, Loss：2.979771852493286\n",
      "49.44169998168945\t 训练次数：8700, Loss：2.8634328842163086\n",
      "---------- EPOCH 21 ----------\n",
      "50.0219042301178\t 训练次数：8800, Loss：2.978156089782715\n",
      "50.58831548690796\t 训练次数：8900, Loss：2.9317593574523926\n",
      "51.14175629615784\t 训练次数：9000, Loss：2.9315199851989746\n",
      "51.69461917877197\t 训练次数：9100, Loss：2.909882068634033\n",
      "---------- EPOCH 22 ----------\n",
      "\n",
      "52.24146890640259\t 训练次数：9200, Loss：2.907095193862915\n",
      "52.794612884521484\t 训练次数：9300, Loss：2.9344937801361084\n",
      "53.334062814712524\t 训练次数：9400, Loss：2.9217376708984375\n",
      "53.88023257255554\t 训练次数：9500, Loss：3.048442840576172\n",
      "54.447996616363525\t 训练次数：9600, Loss：2.801284074783325\n",
      "---------- EPOCH 23 ----------\n",
      "55.00148677825928\t 训练次数：9700, Loss：2.8672964572906494\n",
      "55.575634479522705\t 训练次数：9800, Loss：2.9823997020721436\n",
      "56.11200761795044\t 训练次数：9900, Loss：3.0165932178497314\n",
      "56.68386435508728\t 训练次数：10000, Loss：2.9070887565612793\n",
      "---------- EPOCH 24 ----------\n",
      "\n",
      "57.25461173057556\t 训练次数：10100, Loss：2.8481996059417725\n",
      "57.809821367263794\t 训练次数：10200, Loss：2.925827980041504\n",
      "58.35620880126953\t 训练次数：10300, Loss：2.9727494716644287\n",
      "58.90863919258118\t 训练次数：10400, Loss：2.99834942817688\n",
      "59.4596266746521\t 训练次数：10500, Loss：2.905143976211548\n",
      "---------- EPOCH 25 ----------\n",
      "60.02904224395752\t 训练次数：10600, Loss：2.844578504562378\n",
      "60.59263372421265\t 训练次数：10700, Loss：2.8270537853240967\n",
      "61.17364716529846\t 训练次数：10800, Loss：2.8649473190307617\n",
      "61.78041887283325\t 训练次数：10900, Loss：2.857586622238159\n",
      "---------- EPOCH 26 ----------\n",
      "62.36125993728638\t 训练次数：11000, Loss：3.1422173976898193\n",
      "62.95280122756958\t 训练次数：11100, Loss：2.94631290435791\n",
      "63.614861488342285\t 训练次数：11200, Loss：2.870314598083496\n",
      "64.20197463035583\t 训练次数：11300, Loss：3.112027645111084\n",
      "---------- EPOCH 27 ----------\n",
      "\n",
      "64.81797456741333\t 训练次数：11400, Loss：2.8678579330444336\n",
      "65.3771424293518\t 训练次数：11500, Loss：2.737494468688965\n",
      "65.97069931030273\t 训练次数：11600, Loss：2.9537341594696045\n",
      "66.5780975818634\t 训练次数：11700, Loss：3.0247044563293457\n",
      "67.19131755828857\t 训练次数：11800, Loss：2.8788843154907227\n",
      "---------- EPOCH 28 ----------\n",
      "67.80078053474426\t 训练次数：11900, Loss：2.942354440689087\n",
      "68.90409851074219\t 训练次数：12000, Loss：2.9543073177337646\n",
      "70.11596655845642\t 训练次数：12100, Loss：2.948082447052002\n",
      "71.28424572944641\t 训练次数：12200, Loss：2.909752368927002\n",
      "---------- EPOCH 29 ----------\n",
      "72.45814204216003\t 训练次数：12300, Loss：2.8512697219848633\n",
      "73.61325764656067\t 训练次数：12400, Loss：2.826441764831543\n",
      "74.79859185218811\t 训练次数：12500, Loss：2.9388163089752197\n",
      "75.98908352851868\t 训练次数：12600, Loss：2.9874978065490723\n",
      "77.19393610954285\t 训练次数：12700, Loss：2.8542768955230713\n",
      "---------- EPOCH 30 ----------\n",
      "78.3771288394928\t 训练次数：12800, Loss：2.7470932006835938\n",
      "79.57351970672607\t 训练次数：12900, Loss：2.8417606353759766\n",
      "80.75733208656311\t 训练次数：13000, Loss：3.0048279762268066\n",
      "81.95277667045593\t 训练次数：13100, Loss：2.9607691764831543\n",
      "---------- EPOCH 31 ----------\n",
      "83.17385745048523\t 训练次数：13200, Loss：3.0834286212921143\n",
      "84.36339449882507\t 训练次数：13300, Loss：2.8101987838745117\n",
      "85.54953193664551\t 训练次数：13400, Loss：2.7560267448425293\n",
      "86.8454601764679\t 训练次数：13500, Loss：2.9304442405700684\n",
      "---------- EPOCH 32 ----------\n",
      "88.05109667778015\t 训练次数：13600, Loss：3.022921323776245\n",
      "89.26094841957092\t 训练次数：13700, Loss：3.029811143875122\n",
      "90.4677562713623\t 训练次数：13800, Loss：3.0648646354675293\n",
      "91.70571565628052\t 训练次数：13900, Loss：2.846839666366577\n",
      "92.93245077133179\t 训练次数：14000, Loss：2.846071243286133\n",
      "---------- EPOCH 33 ----------\n",
      "94.14744162559509\t 训练次数：14100, Loss：2.9946541786193848\n",
      "95.39647245407104\t 训练次数：14200, Loss：2.9139294624328613\n",
      "96.58427739143372\t 训练次数：14300, Loss：3.004761219024658\n",
      "97.79664874076843\t 训练次数：14400, Loss：2.902750253677368\n",
      "---------- EPOCH 34 ----------\n",
      "98.97822856903076\t 训练次数：14500, Loss：2.8321893215179443\n",
      "100.17696499824524\t 训练次数：14600, Loss：3.056328773498535\n",
      "101.37363290786743\t 训练次数：14700, Loss：2.862232208251953\n",
      "102.62987303733826\t 训练次数：14800, Loss：2.8008956909179688\n",
      "---------- EPOCH 35 ----------\n",
      "\n",
      "103.77664470672607\t 训练次数：14900, Loss：2.937133312225342\n",
      "104.95778155326843\t 训练次数：15000, Loss：2.8202013969421387\n",
      "106.13013958930969\t 训练次数：15100, Loss：2.862579584121704\n",
      "107.28054523468018\t 训练次数：15200, Loss：2.993377208709717\n",
      "108.47473812103271\t 训练次数：15300, Loss：2.8670687675476074\n",
      "---------- EPOCH 36 ----------\n",
      "109.67268252372742\t 训练次数：15400, Loss：2.7655704021453857\n",
      "110.87071347236633\t 训练次数：15500, Loss：2.9735987186431885\n",
      "112.06490015983582\t 训练次数：15600, Loss：2.8907458782196045\n",
      "113.23262310028076\t 训练次数：15700, Loss：2.87764573097229\n",
      "---------- EPOCH 37 ----------\n",
      "114.39541935920715\t 训练次数：15800, Loss：3.006030321121216\n",
      "115.51739120483398\t 训练次数：15900, Loss：2.8516783714294434\n",
      "116.66087102890015\t 训练次数：16000, Loss：2.8387856483459473\n",
      "117.91714525222778\t 训练次数：16100, Loss：2.8831937313079834\n",
      "119.10872316360474\t 训练次数：16200, Loss：2.879539728164673\n",
      "---------- EPOCH 38 ----------\n",
      "120.32034015655518\t 训练次数：16300, Loss：2.883906126022339\n",
      "121.5097005367279\t 训练次数：16400, Loss：2.819383144378662\n",
      "122.70942735671997\t 训练次数：16500, Loss：2.993898391723633\n",
      "123.88226532936096\t 训练次数：16600, Loss：2.9358901977539062\n",
      "---------- EPOCH 39 ----------\n",
      "125.01939702033997\t 训练次数：16700, Loss：3.025498390197754\n",
      "126.19217610359192\t 训练次数：16800, Loss：2.862520694732666\n",
      "127.36423993110657\t 训练次数：16900, Loss：2.9393577575683594\n",
      "128.50860476493835\t 训练次数：17000, Loss：2.8246042728424072\n",
      "---------- EPOCH 40 ----------\n",
      "\n",
      "129.7016921043396\t 训练次数：17100, Loss：3.077441692352295\n",
      "130.87066102027893\t 训练次数：17200, Loss：2.902212381362915\n",
      "131.97575569152832\t 训练次数：17300, Loss：2.963775873184204\n",
      "133.09663820266724\t 训练次数：17400, Loss：2.8852086067199707\n",
      "134.25324726104736\t 训练次数：17500, Loss：2.904383420944214\n",
      "---------- EPOCH 41 ----------\n",
      "135.41231441497803\t 训练次数：17600, Loss：2.794534683227539\n",
      "136.55127334594727\t 训练次数：17700, Loss：2.8782153129577637\n",
      "137.6904513835907\t 训练次数：17800, Loss：2.986826181411743\n",
      "138.8180959224701\t 训练次数：17900, Loss：2.8897671699523926\n",
      "---------- EPOCH 42 ----------\n",
      "139.93447017669678\t 训练次数：18000, Loss：2.8976340293884277\n",
      "141.0811731815338\t 训练次数：18100, Loss：2.8997042179107666\n",
      "142.25662422180176\t 训练次数：18200, Loss：2.893686056137085\n",
      "143.49474549293518\t 训练次数：18300, Loss：2.8586924076080322\n",
      "---------- EPOCH 43 ----------\n",
      "\n",
      "144.71878004074097\t 训练次数：18400, Loss：2.926072359085083\n",
      "145.87292957305908\t 训练次数：18500, Loss：3.0317585468292236\n",
      "147.10909056663513\t 训练次数：18600, Loss：2.902858257293701\n",
      "148.32380056381226\t 训练次数：18700, Loss：2.936363697052002\n",
      "149.54191303253174\t 训练次数：18800, Loss：2.845525026321411\n",
      "---------- EPOCH 44 ----------\n",
      "150.72952222824097\t 训练次数：18900, Loss：3.0339772701263428\n",
      "151.93970108032227\t 训练次数：19000, Loss：2.9342265129089355\n",
      "153.0848376750946\t 训练次数：19100, Loss：3.0192384719848633\n",
      "154.28049111366272\t 训练次数：19200, Loss：2.917421579360962\n",
      "---------- EPOCH 45 ----------\n",
      "155.47604274749756\t 训练次数：19300, Loss：2.932281017303467\n",
      "156.6596827507019\t 训练次数：19400, Loss：2.8172900676727295\n",
      "157.86218643188477\t 训练次数：19500, Loss：2.9345972537994385\n",
      "159.09647727012634\t 训练次数：19600, Loss：2.908874273300171\n",
      "160.32832264900208\t 训练次数：19700, Loss：2.956017255783081\n",
      "---------- EPOCH 46 ----------\n",
      "161.54247164726257\t 训练次数：19800, Loss：2.895665407180786\n",
      "162.71116852760315\t 训练次数：19900, Loss：2.8887860774993896\n",
      "163.8958010673523\t 训练次数：20000, Loss：2.9076344966888428\n",
      "165.0898518562317\t 训练次数：20100, Loss：2.882490873336792\n",
      "---------- EPOCH 47 ----------\n",
      "166.25861859321594\t 训练次数：20200, Loss：2.7535598278045654\n",
      "167.41685914993286\t 训练次数：20300, Loss：2.865734338760376\n",
      "168.6051688194275\t 训练次数：20400, Loss：2.801882028579712\n",
      "169.83525848388672\t 训练次数：20500, Loss：2.843595504760742\n",
      "---------- EPOCH 48 ----------\n",
      "\n",
      "170.98470902442932\t 训练次数：20600, Loss：2.89414119720459\n",
      "172.18508863449097\t 训练次数：20700, Loss：2.9959208965301514\n",
      "173.38516855239868\t 训练次数：20800, Loss：2.794973134994507\n",
      "174.55309581756592\t 训练次数：20900, Loss：2.861513614654541\n",
      "175.71089363098145\t 训练次数：21000, Loss：3.0724987983703613\n",
      "---------- EPOCH 49 ----------\n",
      "176.8790638446808\t 训练次数：21100, Loss：2.876770257949829\n",
      "178.06215023994446\t 训练次数：21200, Loss：2.8000853061676025\n",
      "179.261785030365\t 训练次数：21300, Loss：2.893197774887085\n",
      "180.44145131111145\t 训练次数：21400, Loss：2.8076858520507812\n",
      "---------- EPOCH 50 ----------\n",
      "181.627179145813\t 训练次数：21500, Loss：2.7964062690734863\n",
      "182.8005301952362\t 训练次数：21600, Loss：2.8738698959350586\n",
      "183.97222113609314\t 训练次数：21700, Loss：2.8224868774414062\n",
      "185.16245555877686\t 训练次数：21800, Loss：2.798978090286255\n",
      "186.3487105369568\t 训练次数：21900, Loss：2.8496649265289307\n",
      "---------- EPOCH 51 ----------\n",
      "187.53619074821472\t 训练次数：22000, Loss：2.953796148300171\n",
      "188.70322799682617\t 训练次数：22100, Loss：3.0900862216949463\n",
      "189.87127089500427\t 训练次数：22200, Loss：2.8070578575134277\n",
      "191.0749146938324\t 训练次数：22300, Loss：2.9641551971435547\n",
      "---------- EPOCH 52 ----------\n",
      "192.28279376029968\t 训练次数：22400, Loss：2.8581809997558594\n",
      "193.58223056793213\t 训练次数：22500, Loss：2.8457441329956055\n",
      "194.79855847358704\t 训练次数：22600, Loss：3.0427613258361816\n",
      "196.00704908370972\t 训练次数：22700, Loss：2.8795435428619385\n",
      "---------- EPOCH 53 ----------\n",
      "197.185307264328\t 训练次数：22800, Loss：2.795822858810425\n",
      "198.38258624076843\t 训练次数：22900, Loss：2.8506832122802734\n",
      "199.60125303268433\t 训练次数：23000, Loss：2.9080400466918945\n",
      "200.79839158058167\t 训练次数：23100, Loss：2.8465139865875244\n",
      "201.9871814250946\t 训练次数：23200, Loss：2.8284912109375\n",
      "---------- EPOCH 54 ----------\n",
      "203.20196294784546\t 训练次数：23300, Loss：2.8138062953948975\n",
      "204.42518615722656\t 训练次数：23400, Loss：2.8940465450286865\n",
      "205.764657497406\t 训练次数：23500, Loss：2.8556067943573\n",
      "207.00745153427124\t 训练次数：23600, Loss：2.8422389030456543\n",
      "---------- EPOCH 55 ----------\n",
      "208.1770224571228\t 训练次数：23700, Loss：2.7737855911254883\n",
      "209.3703737258911\t 训练次数：23800, Loss：2.980255603790283\n",
      "210.54142451286316\t 训练次数：23900, Loss：2.8688650131225586\n",
      "211.72472023963928\t 训练次数：24000, Loss：2.926862955093384\n",
      "---------- EPOCH 56 ----------\n",
      "\n",
      "212.9317982196808\t 训练次数：24100, Loss：2.930575132369995\n",
      "214.16154789924622\t 训练次数：24200, Loss：2.8487534523010254\n",
      "215.38633823394775\t 训练次数：24300, Loss：2.8229427337646484\n",
      "216.60738348960876\t 训练次数：24400, Loss：2.9771296977996826\n",
      "217.8354890346527\t 训练次数：24500, Loss：2.906729221343994\n",
      "---------- EPOCH 57 ----------\n",
      "219.05650281906128\t 训练次数：24600, Loss：2.843008518218994\n",
      "220.2669472694397\t 训练次数：24700, Loss：2.802032947540283\n",
      "221.45067715644836\t 训练次数：24800, Loss：2.819643259048462\n",
      "222.66007089614868\t 训练次数：24900, Loss：2.830449104309082\n",
      "---------- EPOCH 58 ----------\n",
      "223.86704230308533\t 训练次数：25000, Loss：2.9366769790649414\n",
      "225.0652916431427\t 训练次数：25100, Loss：2.885089874267578\n",
      "226.2644340991974\t 训练次数：25200, Loss：2.915437698364258\n",
      "227.44660711288452\t 训练次数：25300, Loss：2.8606457710266113\n",
      "228.62381649017334\t 训练次数：25400, Loss：2.8212361335754395\n",
      "---------- EPOCH 59 ----------\n",
      "229.79894971847534\t 训练次数：25500, Loss：2.739365577697754\n",
      "231.02884030342102\t 训练次数：25600, Loss：2.794412136077881\n",
      "232.25363111495972\t 训练次数：25700, Loss：2.9099953174591064\n",
      "233.51128602027893\t 训练次数：25800, Loss：3.007028341293335\n",
      "---------- EPOCH 60 ----------\n",
      "234.71534824371338\t 训练次数：25900, Loss：2.864034652709961\n",
      "235.8885850906372\t 训练次数：26000, Loss：2.90732741355896\n",
      "237.10754370689392\t 训练次数：26100, Loss：2.7649168968200684\n",
      "238.28506922721863\t 训练次数：26200, Loss：2.8178536891937256\n",
      "---------- EPOCH 61 ----------\n",
      "239.5164773464203\t 训练次数：26300, Loss：2.7817976474761963\n",
      "240.73857474327087\t 训练次数：26400, Loss：2.845935583114624\n",
      "241.96359992027283\t 训练次数：26500, Loss：2.8149526119232178\n",
      "243.13503432273865\t 训练次数：26600, Loss：2.9763307571411133\n",
      "244.3766314983368\t 训练次数：26700, Loss：2.8043212890625\n",
      "---------- EPOCH 62 ----------\n",
      "245.60528707504272\t 训练次数：26800, Loss：2.866332530975342\n",
      "246.795254945755\t 训练次数：26900, Loss：2.856278896331787\n",
      "247.9860429763794\t 训练次数：27000, Loss：2.791656970977783\n",
      "249.19369435310364\t 训练次数：27100, Loss：2.9381706714630127\n",
      "---------- EPOCH 63 ----------\n",
      "250.4227225780487\t 训练次数：27200, Loss：2.8466949462890625\n",
      "251.64320039749146\t 训练次数：27300, Loss：2.783278226852417\n",
      "252.87699437141418\t 训练次数：27400, Loss：2.8459832668304443\n",
      "254.08589339256287\t 训练次数：27500, Loss：2.82658314704895\n",
      "---------- EPOCH 64 ----------\n",
      "\n",
      "255.30326581001282\t 训练次数：27600, Loss：2.85428786277771\n",
      "256.49964809417725\t 训练次数：27700, Loss：3.0413904190063477\n",
      "257.76344752311707\t 训练次数：27800, Loss：2.8122997283935547\n",
      "258.9754295349121\t 训练次数：27900, Loss：2.8686366081237793\n",
      "260.17603302001953\t 训练次数：28000, Loss：2.8892266750335693\n",
      "---------- EPOCH 65 ----------\n",
      "261.3851761817932\t 训练次数：28100, Loss：2.7599189281463623\n",
      "262.58963990211487\t 训练次数：28200, Loss：2.891965866088867\n",
      "263.77995800971985\t 训练次数：28300, Loss：2.825270652770996\n",
      "264.95176458358765\t 训练次数：28400, Loss：2.7984349727630615\n",
      "---------- EPOCH 66 ----------\n",
      "266.1554310321808\t 训练次数：28500, Loss：2.799776315689087\n",
      "267.356027841568\t 训练次数：28600, Loss：2.891374111175537\n",
      "268.57607913017273\t 训练次数：28700, Loss：2.8935959339141846\n",
      "269.7684805393219\t 训练次数：28800, Loss：2.811100721359253\n",
      "270.98968052864075\t 训练次数：28900, Loss：2.8615593910217285\n",
      "---------- EPOCH 67 ----------\n",
      "272.203293800354\t 训练次数：29000, Loss：2.881761312484741\n",
      "273.44743037223816\t 训练次数：29100, Loss：2.795377254486084\n",
      "274.6712188720703\t 训练次数：29200, Loss：2.9068896770477295\n",
      "275.88696479797363\t 训练次数：29300, Loss：2.8673834800720215\n",
      "---------- EPOCH 68 ----------\n",
      "277.11053133010864\t 训练次数：29400, Loss：2.8733935356140137\n",
      "278.29067039489746\t 训练次数：29500, Loss：2.8149232864379883\n",
      "279.4700710773468\t 训练次数：29600, Loss：2.933985471725464\n",
      "280.6762080192566\t 训练次数：29700, Loss：2.943079948425293\n",
      "---------- EPOCH 69 ----------\n",
      "\n",
      "281.87478947639465\t 训练次数：29800, Loss：2.7671852111816406\n",
      "283.08138394355774\t 训练次数：29900, Loss：2.9257454872131348\n",
      "284.2611620426178\t 训练次数：30000, Loss：2.817648410797119\n",
      "285.465927362442\t 训练次数：30100, Loss：2.824282646179199\n",
      "286.6758244037628\t 训练次数：30200, Loss：2.7966291904449463\n",
      "---------- EPOCH 70 ----------\n",
      "287.87998247146606\t 训练次数：30300, Loss：2.8638288974761963\n",
      "289.05436182022095\t 训练次数：30400, Loss：2.7924909591674805\n",
      "290.2607796192169\t 训练次数：30500, Loss：2.7340431213378906\n",
      "291.469603061676\t 训练次数：30600, Loss：2.9882097244262695\n",
      "---------- EPOCH 71 ----------\n",
      "292.694043636322\t 训练次数：30700, Loss：2.8148322105407715\n",
      "293.8910710811615\t 训练次数：30800, Loss：2.883180856704712\n",
      "295.0999393463135\t 训练次数：30900, Loss：2.978297710418701\n",
      "296.30022072792053\t 训练次数：31000, Loss：2.865762233734131\n",
      "---------- EPOCH 72 ----------\n",
      "\n",
      "297.5047435760498\t 训练次数：31100, Loss：2.8765735626220703\n",
      "298.6836893558502\t 训练次数：31200, Loss：2.874354839324951\n",
      "299.84808230400085\t 训练次数：31300, Loss：2.837355375289917\n",
      "301.0159306526184\t 训练次数：31400, Loss：3.0449087619781494\n",
      "302.19236063957214\t 训练次数：31500, Loss：2.8390326499938965\n",
      "---------- EPOCH 73 ----------\n",
      "303.36937522888184\t 训练次数：31600, Loss：2.8106822967529297\n",
      "304.5757727622986\t 训练次数：31700, Loss：2.7785348892211914\n",
      "305.75062799453735\t 训练次数：31800, Loss：2.7446088790893555\n",
      "306.93570923805237\t 训练次数：31900, Loss：2.8316192626953125\n",
      "---------- EPOCH 74 ----------\n",
      "308.13343954086304\t 训练次数：32000, Loss：2.880901575088501\n",
      "309.2992651462555\t 训练次数：32100, Loss：2.825496196746826\n",
      "310.4973545074463\t 训练次数：32200, Loss：2.894601821899414\n",
      "311.70983266830444\t 训练次数：32300, Loss：2.866865396499634\n",
      "312.90146470069885\t 训练次数：32400, Loss：2.824078321456909\n",
      "---------- EPOCH 75 ----------\n",
      "314.1064043045044\t 训练次数：32500, Loss：2.979963779449463\n",
      "315.33510184288025\t 训练次数：32600, Loss：2.8907651901245117\n",
      "316.57099986076355\t 训练次数：32700, Loss：2.8368923664093018\n",
      "317.75800108909607\t 训练次数：32800, Loss：2.909475326538086\n",
      "---------- EPOCH 76 ----------\n",
      "318.973530292511\t 训练次数：32900, Loss：2.920469284057617\n",
      "320.16126132011414\t 训练次数：33000, Loss：2.9951179027557373\n",
      "321.45006012916565\t 训练次数：33100, Loss：2.8048415184020996\n",
      "322.6774961948395\t 训练次数：33200, Loss：2.7367899417877197\n",
      "---------- EPOCH 77 ----------\n",
      "\n",
      "323.90374517440796\t 训练次数：33300, Loss：2.8323757648468018\n",
      "325.07371616363525\t 训练次数：33400, Loss：2.811187982559204\n",
      "326.2629179954529\t 训练次数：33500, Loss：3.0818493366241455\n",
      "327.48808574676514\t 训练次数：33600, Loss：2.8314590454101562\n",
      "328.7072124481201\t 训练次数：33700, Loss：2.7579827308654785\n",
      "---------- EPOCH 78 ----------\n",
      "329.918582201004\t 训练次数：33800, Loss：2.8378429412841797\n",
      "331.1216127872467\t 训练次数：33900, Loss：2.7965219020843506\n",
      "332.352511882782\t 训练次数：34000, Loss：2.896953582763672\n",
      "333.5440745353699\t 训练次数：34100, Loss：2.823742628097534\n",
      "---------- EPOCH 79 ----------\n",
      "334.80751514434814\t 训练次数：34200, Loss：2.7360641956329346\n",
      "336.0447373390198\t 训练次数：34300, Loss：2.849193572998047\n",
      "337.2513339519501\t 训练次数：34400, Loss：2.793665885925293\n",
      "338.43991923332214\t 训练次数：34500, Loss：2.843682289123535\n",
      "339.6041679382324\t 训练次数：34600, Loss：2.8417911529541016\n",
      "---------- EPOCH 80 ----------\n",
      "340.80140948295593\t 训练次数：34700, Loss：2.8330092430114746\n",
      "341.96671962738037\t 训练次数：34800, Loss：2.8506791591644287\n",
      "343.1380729675293\t 训练次数：34900, Loss：2.862769603729248\n",
      "344.3248088359833\t 训练次数：35000, Loss：3.0032989978790283\n",
      "---------- EPOCH 81 ----------\n",
      "345.5188663005829\t 训练次数：35100, Loss：2.8059070110321045\n",
      "346.7144091129303\t 训练次数：35200, Loss：2.9097073078155518\n",
      "347.9114592075348\t 训练次数：35300, Loss：2.7722651958465576\n",
      "349.1227867603302\t 训练次数：35400, Loss：2.948481798171997\n",
      "---------- EPOCH 82 ----------\n",
      "350.31045413017273\t 训练次数：35500, Loss：2.749044895172119\n",
      "351.4597964286804\t 训练次数：35600, Loss：2.9428160190582275\n",
      "352.65504217147827\t 训练次数：35700, Loss：2.9175617694854736\n",
      "353.86753726005554\t 训练次数：35800, Loss：2.8173460960388184\n",
      "355.0063474178314\t 训练次数：35900, Loss：3.041980504989624\n",
      "---------- EPOCH 83 ----------\n",
      "356.19200897216797\t 训练次数：36000, Loss：2.8130722045898438\n",
      "357.3883981704712\t 训练次数：36100, Loss：2.8596556186676025\n",
      "358.5634272098541\t 训练次数：36200, Loss：2.747039556503296\n",
      "359.76758551597595\t 训练次数：36300, Loss：2.8496954441070557\n",
      "---------- EPOCH 84 ----------\n",
      "360.9128966331482\t 训练次数：36400, Loss：2.8791964054107666\n",
      "362.10654282569885\t 训练次数：36500, Loss：2.9005939960479736\n",
      "363.3126537799835\t 训练次数：36600, Loss：2.740184783935547\n",
      "364.501571893692\t 训练次数：36700, Loss：2.861130714416504\n",
      "---------- EPOCH 85 ----------\n",
      "\n",
      "365.7247226238251\t 训练次数：36800, Loss：2.8667421340942383\n",
      "366.9150643348694\t 训练次数：36900, Loss：2.955827236175537\n",
      "368.14654326438904\t 训练次数：37000, Loss：2.818044424057007\n",
      "369.3341956138611\t 训练次数：37100, Loss：2.8329596519470215\n",
      "370.54078221321106\t 训练次数：37200, Loss：2.7769291400909424\n",
      "---------- EPOCH 86 ----------\n",
      "371.7031743526459\t 训练次数：37300, Loss：2.9032771587371826\n",
      "372.8985695838928\t 训练次数：37400, Loss：2.849663734436035\n",
      "374.08351612091064\t 训练次数：37500, Loss：2.9931845664978027\n",
      "375.2558403015137\t 训练次数：37600, Loss：2.802527904510498\n",
      "---------- EPOCH 87 ----------\n",
      "376.40051531791687\t 训练次数：37700, Loss：2.963672161102295\n",
      "377.58996081352234\t 训练次数：37800, Loss：2.735262155532837\n",
      "378.7702922821045\t 训练次数：37900, Loss：2.8034861087799072\n",
      "379.95762395858765\t 训练次数：38000, Loss：2.880945920944214\n",
      "381.13454699516296\t 训练次数：38100, Loss：2.9432737827301025\n",
      "---------- EPOCH 88 ----------\n",
      "382.32294273376465\t 训练次数：38200, Loss：2.860983371734619\n",
      "383.50733757019043\t 训练次数：38300, Loss：2.891580581665039\n",
      "384.710480928421\t 训练次数：38400, Loss：2.8595516681671143\n",
      "385.88651418685913\t 训练次数：38500, Loss：2.792728900909424\n",
      "---------- EPOCH 89 ----------\n",
      "387.0756092071533\t 训练次数：38600, Loss：2.789985418319702\n",
      "388.2594528198242\t 训练次数：38700, Loss：2.7856836318969727\n",
      "389.422486782074\t 训练次数：38800, Loss：2.9664390087127686\n",
      "390.5657651424408\t 训练次数：38900, Loss：2.9358623027801514\n",
      "---------- EPOCH 90 ----------\n",
      "\n",
      "391.7046949863434\t 训练次数：39000, Loss：2.840064287185669\n",
      "392.8694221973419\t 训练次数：39100, Loss：2.853944778442383\n",
      "394.0633661746979\t 训练次数：39200, Loss：2.7797038555145264\n",
      "395.239453792572\t 训练次数：39300, Loss：2.732455253601074\n",
      "396.41703391075134\t 训练次数：39400, Loss：2.7492566108703613\n",
      "---------- EPOCH 91 ----------\n",
      "397.63631319999695\t 训练次数：39500, Loss：2.8392274379730225\n",
      "398.8108172416687\t 训练次数：39600, Loss：2.901644706726074\n",
      "400.02913308143616\t 训练次数：39700, Loss：2.765289783477783\n",
      "401.20557832717896\t 训练次数：39800, Loss：2.8385074138641357\n",
      "---------- EPOCH 92 ----------\n",
      "402.4144973754883\t 训练次数：39900, Loss：2.922605514526367\n",
      "403.6165702342987\t 训练次数：40000, Loss：2.8522181510925293\n",
      "404.7917423248291\t 训练次数：40100, Loss：2.78316593170166\n",
      "405.99329566955566\t 训练次数：40200, Loss：2.9151217937469482\n",
      "---------- EPOCH 93 ----------\n",
      "\n",
      "407.1964523792267\t 训练次数：40300, Loss：2.76055908203125\n",
      "408.361172914505\t 训练次数：40400, Loss：2.774381637573242\n",
      "409.5215907096863\t 训练次数：40500, Loss：2.7632501125335693\n",
      "410.71983575820923\t 训练次数：40600, Loss：2.8789806365966797\n",
      "411.90427446365356\t 训练次数：40700, Loss：2.852388858795166\n",
      "---------- EPOCH 94 ----------\n",
      "413.08421087265015\t 训练次数：40800, Loss：2.759575128555298\n",
      "414.2732651233673\t 训练次数：40900, Loss：2.87349009513855\n",
      "415.4710328578949\t 训练次数：41000, Loss：2.8127684593200684\n",
      "416.6602964401245\t 训练次数：41100, Loss：2.913994073867798\n",
      "---------- EPOCH 95 ----------\n",
      "417.85691833496094\t 训练次数：41200, Loss：2.8553264141082764\n",
      "419.0444087982178\t 训练次数：41300, Loss：2.808915138244629\n",
      "420.21278262138367\t 训练次数：41400, Loss：2.9484236240386963\n",
      "421.37528371810913\t 训练次数：41500, Loss：2.9298765659332275\n",
      "422.55752205848694\t 训练次数：41600, Loss：2.863600730895996\n",
      "---------- EPOCH 96 ----------\n",
      "423.73193621635437\t 训练次数：41700, Loss：2.934258460998535\n",
      "424.95824813842773\t 训练次数：41800, Loss：2.891319990158081\n",
      "426.1319019794464\t 训练次数：41900, Loss：2.803764581680298\n",
      "427.3176865577698\t 训练次数：42000, Loss：2.9338221549987793\n",
      "---------- EPOCH 97 ----------\n",
      "428.52124071121216\t 训练次数：42100, Loss：2.7476918697357178\n",
      "429.7173309326172\t 训练次数：42200, Loss：3.0416259765625\n",
      "430.905641078949\t 训练次数：42300, Loss：2.869342088699341\n",
      "432.05298709869385\t 训练次数：42400, Loss：2.8694143295288086\n",
      "---------- EPOCH 98 ----------\n",
      "\n",
      "433.21525621414185\t 训练次数：42500, Loss：2.8828988075256348\n",
      "434.3763394355774\t 训练次数：42600, Loss：2.881436824798584\n",
      "435.525390625\t 训练次数：42700, Loss：2.872575521469116\n",
      "436.6892294883728\t 训练次数：42800, Loss：2.7372074127197266\n",
      "437.84647369384766\t 训练次数：42900, Loss：2.816742181777954\n",
      "---------- EPOCH 99 ----------\n",
      "439.02662682533264\t 训练次数：43000, Loss：2.8140645027160645\n",
      "440.2096643447876\t 训练次数：43100, Loss：2.7367820739746094\n",
      "441.39471340179443\t 训练次数：43200, Loss：2.8414580821990967\n",
      "442.6044383049011\t 训练次数：43300, Loss：2.85494065284729\n",
      "---------- EPOCH 100 ----------\n",
      "443.7708296775818\t 训练次数：43400, Loss：2.8400042057037354\n",
      "444.95868015289307\t 训练次数：43500, Loss：2.793095350265503\n",
      "446.15450620651245\t 训练次数：43600, Loss：2.7480978965759277\n",
      "447.3343241214752\t 训练次数：43700, Loss：2.913635730743408\n",
      "448.50453543663025\t 训练次数：43800, Loss：2.81312894821167\n",
      "Test Accuracy: 0.9143\n",
      "模型已保存\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# tensorboard 记录训练结果\n",
    "writer = SummaryWriter(\"./logs_train\")\n",
    "\n",
    "# dataloader\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "# 损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, foreach=False)\n",
    "\n",
    "# Training Loop\n",
    "start_time = time.time()\n",
    "total_train_step = 0\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(f\"---------- EPOCH {epoch+1} ----------\\n\")\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # 优化器清除梯度\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # 交叉熵计算损失\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        # 优化器优化模型\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 误差分析\n",
    "        total_train_step += 1\n",
    "        if total_train_step % 100 == 0:\n",
    "            end_time = time.time()\n",
    "            print(f\"{end_time-start_time}\\t 训练次数：{total_train_step}, Loss：{loss}\")\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "            \n",
    "# 测试\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_correct = 0        \n",
    "    total_samples = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        values, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += len(labels)\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    torch.save(model, f\".\\Weights/eeg_gpu.pth\")\n",
    "    # torch.save(light.state_dict(), f\"Weights/light_{epoch}.pth\")\n",
    "    print(\"模型已保存\")\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:28:59.688133200Z",
     "start_time": "2024-02-26T04:21:30.338296900Z"
    }
   },
   "id": "4b19576f5a7dabfd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 测试集\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        values, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += len(labels)\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    torch.save(model, f\".\\Weights/eeg_gpu.pth\")\n",
    "    # torch.save(light.state_dict(), f\"Weights/light_{epoch}.pth\")\n",
    "    print(\"模型已保存\")\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2c56d44a0b8c4e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "57e523c0f932d833"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "91ac362022690b1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d5946e52139ccfb3"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: torch.Size([16, 1, 9, 1375])\n",
      "1: torch.Size([16, 1, 9, 1375])\n",
      "2: torch.Size([16, 1, 9, 1375])\n",
      "3: torch.Size([16, 1, 9, 1375])\n",
      "4: torch.Size([16, 1, 9, 1375])\n",
      "5: torch.Size([16, 1, 9, 1375])\n",
      "6: torch.Size([16, 1, 9, 1375])\n",
      "7: torch.Size([16, 1, 9, 1375])\n",
      "8: torch.Size([16, 1, 9, 1375])\n",
      "9: torch.Size([16, 1, 9, 1375])\n",
      "10: torch.Size([16, 1, 9, 1375])\n",
      "11: torch.Size([16, 1, 9, 1375])\n",
      "12: torch.Size([16, 1, 9, 1375])\n",
      "13: torch.Size([16, 1, 9, 1375])\n",
      "14: torch.Size([16, 1, 9, 1375])\n",
      "15: torch.Size([16, 1, 9, 1375])\n",
      "16: torch.Size([16, 1, 9, 1375])\n",
      "17: torch.Size([16, 1, 9, 1375])\n",
      "18: torch.Size([16, 1, 9, 1375])\n",
      "19: torch.Size([16, 1, 9, 1375])\n",
      "20: torch.Size([16, 1, 9, 1375])\n",
      "21: torch.Size([16, 1, 9, 1375])\n",
      "22: torch.Size([16, 1, 9, 1375])\n",
      "23: torch.Size([16, 1, 9, 1375])\n",
      "24: torch.Size([16, 1, 9, 1375])\n",
      "25: torch.Size([16, 1, 9, 1375])\n",
      "26: torch.Size([16, 1, 9, 1375])\n",
      "27: torch.Size([16, 1, 9, 1375])\n",
      "28: torch.Size([16, 1, 9, 1375])\n",
      "29: torch.Size([16, 1, 9, 1375])\n",
      "30: torch.Size([16, 1, 9, 1375])\n",
      "31: torch.Size([16, 1, 9, 1375])\n",
      "32: torch.Size([16, 1, 9, 1375])\n",
      "33: torch.Size([16, 1, 9, 1375])\n",
      "34: torch.Size([16, 1, 9, 1375])\n",
      "35: torch.Size([16, 1, 9, 1375])\n",
      "36: torch.Size([16, 1, 9, 1375])\n",
      "37: torch.Size([16, 1, 9, 1375])\n",
      "38: torch.Size([16, 1, 9, 1375])\n",
      "39: torch.Size([16, 1, 9, 1375])\n",
      "40: torch.Size([16, 1, 9, 1375])\n",
      "41: torch.Size([16, 1, 9, 1375])\n",
      "42: torch.Size([16, 1, 9, 1375])\n",
      "43: torch.Size([16, 1, 9, 1375])\n",
      "44: torch.Size([16, 1, 9, 1375])\n",
      "45: torch.Size([16, 1, 9, 1375])\n",
      "46: torch.Size([16, 1, 9, 1375])\n",
      "47: torch.Size([16, 1, 9, 1375])\n",
      "48: torch.Size([16, 1, 9, 1375])\n",
      "49: torch.Size([16, 1, 9, 1375])\n",
      "50: torch.Size([16, 1, 9, 1375])\n",
      "51: torch.Size([16, 1, 9, 1375])\n",
      "52: torch.Size([16, 1, 9, 1375])\n",
      "53: torch.Size([16, 1, 9, 1375])\n",
      "54: torch.Size([16, 1, 9, 1375])\n",
      "55: torch.Size([16, 1, 9, 1375])\n",
      "56: torch.Size([16, 1, 9, 1375])\n",
      "57: torch.Size([16, 1, 9, 1375])\n",
      "58: torch.Size([16, 1, 9, 1375])\n",
      "59: torch.Size([16, 1, 9, 1375])\n",
      "60: torch.Size([16, 1, 9, 1375])\n",
      "61: torch.Size([16, 1, 9, 1375])\n",
      "62: torch.Size([16, 1, 9, 1375])\n",
      "63: torch.Size([16, 1, 9, 1375])\n",
      "64: torch.Size([16, 1, 9, 1375])\n",
      "65: torch.Size([16, 1, 9, 1375])\n",
      "66: torch.Size([16, 1, 9, 1375])\n",
      "67: torch.Size([16, 1, 9, 1375])\n",
      "68: torch.Size([16, 1, 9, 1375])\n",
      "69: torch.Size([16, 1, 9, 1375])\n",
      "70: torch.Size([16, 1, 9, 1375])\n",
      "71: torch.Size([16, 1, 9, 1375])\n",
      "72: torch.Size([16, 1, 9, 1375])\n",
      "73: torch.Size([16, 1, 9, 1375])\n",
      "74: torch.Size([16, 1, 9, 1375])\n",
      "75: torch.Size([16, 1, 9, 1375])\n",
      "76: torch.Size([16, 1, 9, 1375])\n",
      "77: torch.Size([16, 1, 9, 1375])\n",
      "78: torch.Size([16, 1, 9, 1375])\n",
      "79: torch.Size([16, 1, 9, 1375])\n",
      "80: torch.Size([16, 1, 9, 1375])\n",
      "81: torch.Size([16, 1, 9, 1375])\n",
      "82: torch.Size([16, 1, 9, 1375])\n",
      "83: torch.Size([16, 1, 9, 1375])\n",
      "84: torch.Size([16, 1, 9, 1375])\n",
      "85: torch.Size([16, 1, 9, 1375])\n",
      "86: torch.Size([16, 1, 9, 1375])\n",
      "87: torch.Size([16, 1, 9, 1375])\n",
      "88: torch.Size([16, 1, 9, 1375])\n",
      "89: torch.Size([16, 1, 9, 1375])\n",
      "90: torch.Size([16, 1, 9, 1375])\n",
      "91: torch.Size([16, 1, 9, 1375])\n",
      "92: torch.Size([16, 1, 9, 1375])\n",
      "93: torch.Size([16, 1, 9, 1375])\n",
      "94: torch.Size([16, 1, 9, 1375])\n",
      "95: torch.Size([16, 1, 9, 1375])\n",
      "96: torch.Size([16, 1, 9, 1375])\n",
      "97: torch.Size([16, 1, 9, 1375])\n",
      "98: torch.Size([16, 1, 9, 1375])\n",
      "99: torch.Size([16, 1, 9, 1375])\n",
      "100: torch.Size([16, 1, 9, 1375])\n",
      "101: torch.Size([16, 1, 9, 1375])\n",
      "102: torch.Size([16, 1, 9, 1375])\n",
      "103: torch.Size([16, 1, 9, 1375])\n",
      "104: torch.Size([16, 1, 9, 1375])\n",
      "105: torch.Size([16, 1, 9, 1375])\n",
      "106: torch.Size([16, 1, 9, 1375])\n",
      "107: torch.Size([16, 1, 9, 1375])\n",
      "108: torch.Size([16, 1, 9, 1375])\n",
      "109: torch.Size([16, 1, 9, 1375])\n",
      "110: torch.Size([16, 1, 9, 1375])\n",
      "111: torch.Size([16, 1, 9, 1375])\n",
      "112: torch.Size([16, 1, 9, 1375])\n",
      "113: torch.Size([16, 1, 9, 1375])\n",
      "114: torch.Size([16, 1, 9, 1375])\n",
      "115: torch.Size([16, 1, 9, 1375])\n",
      "116: torch.Size([16, 1, 9, 1375])\n",
      "117: torch.Size([16, 1, 9, 1375])\n",
      "118: torch.Size([16, 1, 9, 1375])\n",
      "119: torch.Size([16, 1, 9, 1375])\n",
      "120: torch.Size([16, 1, 9, 1375])\n",
      "121: torch.Size([16, 1, 9, 1375])\n",
      "122: torch.Size([16, 1, 9, 1375])\n",
      "123: torch.Size([16, 1, 9, 1375])\n",
      "124: torch.Size([16, 1, 9, 1375])\n",
      "125: torch.Size([16, 1, 9, 1375])\n",
      "126: torch.Size([16, 1, 9, 1375])\n",
      "127: torch.Size([16, 1, 9, 1375])\n",
      "128: torch.Size([16, 1, 9, 1375])\n",
      "129: torch.Size([16, 1, 9, 1375])\n",
      "130: torch.Size([16, 1, 9, 1375])\n",
      "131: torch.Size([16, 1, 9, 1375])\n",
      "132: torch.Size([16, 1, 9, 1375])\n",
      "133: torch.Size([16, 1, 9, 1375])\n",
      "134: torch.Size([16, 1, 9, 1375])\n",
      "135: torch.Size([16, 1, 9, 1375])\n",
      "136: torch.Size([16, 1, 9, 1375])\n",
      "137: torch.Size([16, 1, 9, 1375])\n",
      "138: torch.Size([16, 1, 9, 1375])\n",
      "139: torch.Size([16, 1, 9, 1375])\n",
      "140: torch.Size([16, 1, 9, 1375])\n",
      "141: torch.Size([16, 1, 9, 1375])\n",
      "142: torch.Size([16, 1, 9, 1375])\n",
      "143: torch.Size([16, 1, 9, 1375])\n",
      "144: torch.Size([16, 1, 9, 1375])\n",
      "145: torch.Size([16, 1, 9, 1375])\n",
      "146: torch.Size([16, 1, 9, 1375])\n",
      "147: torch.Size([16, 1, 9, 1375])\n",
      "148: torch.Size([16, 1, 9, 1375])\n",
      "149: torch.Size([16, 1, 9, 1375])\n",
      "150: torch.Size([16, 1, 9, 1375])\n",
      "151: torch.Size([16, 1, 9, 1375])\n",
      "152: torch.Size([16, 1, 9, 1375])\n",
      "153: torch.Size([16, 1, 9, 1375])\n",
      "154: torch.Size([16, 1, 9, 1375])\n",
      "155: torch.Size([16, 1, 9, 1375])\n",
      "156: torch.Size([16, 1, 9, 1375])\n",
      "157: torch.Size([16, 1, 9, 1375])\n",
      "158: torch.Size([16, 1, 9, 1375])\n",
      "159: torch.Size([16, 1, 9, 1375])\n",
      "160: torch.Size([16, 1, 9, 1375])\n",
      "161: torch.Size([16, 1, 9, 1375])\n",
      "162: torch.Size([16, 1, 9, 1375])\n",
      "163: torch.Size([16, 1, 9, 1375])\n",
      "164: torch.Size([16, 1, 9, 1375])\n",
      "165: torch.Size([16, 1, 9, 1375])\n",
      "166: torch.Size([16, 1, 9, 1375])\n",
      "167: torch.Size([16, 1, 9, 1375])\n",
      "168: torch.Size([16, 1, 9, 1375])\n",
      "169: torch.Size([16, 1, 9, 1375])\n",
      "170: torch.Size([16, 1, 9, 1375])\n",
      "171: torch.Size([16, 1, 9, 1375])\n",
      "172: torch.Size([16, 1, 9, 1375])\n",
      "173: torch.Size([16, 1, 9, 1375])\n",
      "174: torch.Size([16, 1, 9, 1375])\n",
      "175: torch.Size([16, 1, 9, 1375])\n",
      "176: torch.Size([16, 1, 9, 1375])\n",
      "177: torch.Size([16, 1, 9, 1375])\n",
      "178: torch.Size([16, 1, 9, 1375])\n",
      "179: torch.Size([16, 1, 9, 1375])\n",
      "180: torch.Size([16, 1, 9, 1375])\n",
      "181: torch.Size([16, 1, 9, 1375])\n",
      "182: torch.Size([16, 1, 9, 1375])\n",
      "183: torch.Size([16, 1, 9, 1375])\n",
      "184: torch.Size([16, 1, 9, 1375])\n",
      "185: torch.Size([16, 1, 9, 1375])\n",
      "186: torch.Size([16, 1, 9, 1375])\n",
      "187: torch.Size([16, 1, 9, 1375])\n",
      "188: torch.Size([16, 1, 9, 1375])\n",
      "189: torch.Size([16, 1, 9, 1375])\n",
      "190: torch.Size([16, 1, 9, 1375])\n",
      "191: torch.Size([16, 1, 9, 1375])\n",
      "192: torch.Size([16, 1, 9, 1375])\n",
      "193: torch.Size([16, 1, 9, 1375])\n",
      "194: torch.Size([16, 1, 9, 1375])\n",
      "195: torch.Size([16, 1, 9, 1375])\n",
      "196: torch.Size([16, 1, 9, 1375])\n",
      "197: torch.Size([16, 1, 9, 1375])\n",
      "198: torch.Size([16, 1, 9, 1375])\n",
      "199: torch.Size([16, 1, 9, 1375])\n",
      "200: torch.Size([16, 1, 9, 1375])\n",
      "201: torch.Size([16, 1, 9, 1375])\n",
      "202: torch.Size([16, 1, 9, 1375])\n",
      "203: torch.Size([16, 1, 9, 1375])\n",
      "204: torch.Size([16, 1, 9, 1375])\n",
      "205: torch.Size([16, 1, 9, 1375])\n",
      "206: torch.Size([16, 1, 9, 1375])\n",
      "207: torch.Size([16, 1, 9, 1375])\n",
      "208: torch.Size([16, 1, 9, 1375])\n",
      "209: torch.Size([16, 1, 9, 1375])\n",
      "210: torch.Size([16, 1, 9, 1375])\n",
      "211: torch.Size([16, 1, 9, 1375])\n",
      "212: torch.Size([16, 1, 9, 1375])\n",
      "213: torch.Size([16, 1, 9, 1375])\n",
      "214: torch.Size([16, 1, 9, 1375])\n",
      "215: torch.Size([16, 1, 9, 1375])\n",
      "216: torch.Size([16, 1, 9, 1375])\n",
      "217: torch.Size([16, 1, 9, 1375])\n",
      "218: torch.Size([16, 1, 9, 1375])\n",
      "219: torch.Size([16, 1, 9, 1375])\n",
      "220: torch.Size([16, 1, 9, 1375])\n",
      "221: torch.Size([16, 1, 9, 1375])\n",
      "222: torch.Size([16, 1, 9, 1375])\n",
      "223: torch.Size([16, 1, 9, 1375])\n",
      "224: torch.Size([16, 1, 9, 1375])\n",
      "225: torch.Size([16, 1, 9, 1375])\n",
      "226: torch.Size([16, 1, 9, 1375])\n",
      "227: torch.Size([16, 1, 9, 1375])\n",
      "228: torch.Size([16, 1, 9, 1375])\n",
      "229: torch.Size([16, 1, 9, 1375])\n",
      "230: torch.Size([16, 1, 9, 1375])\n",
      "231: torch.Size([16, 1, 9, 1375])\n",
      "232: torch.Size([16, 1, 9, 1375])\n",
      "233: torch.Size([16, 1, 9, 1375])\n",
      "234: torch.Size([16, 1, 9, 1375])\n",
      "235: torch.Size([16, 1, 9, 1375])\n",
      "236: torch.Size([16, 1, 9, 1375])\n",
      "237: torch.Size([16, 1, 9, 1375])\n",
      "238: torch.Size([16, 1, 9, 1375])\n",
      "239: torch.Size([16, 1, 9, 1375])\n",
      "240: torch.Size([16, 1, 9, 1375])\n",
      "241: torch.Size([16, 1, 9, 1375])\n",
      "242: torch.Size([16, 1, 9, 1375])\n",
      "243: torch.Size([16, 1, 9, 1375])\n",
      "244: torch.Size([16, 1, 9, 1375])\n",
      "245: torch.Size([16, 1, 9, 1375])\n",
      "246: torch.Size([16, 1, 9, 1375])\n",
      "247: torch.Size([16, 1, 9, 1375])\n",
      "248: torch.Size([16, 1, 9, 1375])\n",
      "249: torch.Size([16, 1, 9, 1375])\n",
      "250: torch.Size([16, 1, 9, 1375])\n",
      "251: torch.Size([16, 1, 9, 1375])\n",
      "252: torch.Size([16, 1, 9, 1375])\n",
      "253: torch.Size([16, 1, 9, 1375])\n",
      "254: torch.Size([16, 1, 9, 1375])\n",
      "255: torch.Size([16, 1, 9, 1375])\n",
      "256: torch.Size([16, 1, 9, 1375])\n",
      "257: torch.Size([16, 1, 9, 1375])\n",
      "258: torch.Size([16, 1, 9, 1375])\n",
      "259: torch.Size([16, 1, 9, 1375])\n",
      "260: torch.Size([16, 1, 9, 1375])\n",
      "261: torch.Size([16, 1, 9, 1375])\n",
      "262: torch.Size([16, 1, 9, 1375])\n",
      "263: torch.Size([16, 1, 9, 1375])\n",
      "264: torch.Size([16, 1, 9, 1375])\n",
      "265: torch.Size([16, 1, 9, 1375])\n",
      "266: torch.Size([16, 1, 9, 1375])\n",
      "267: torch.Size([16, 1, 9, 1375])\n",
      "268: torch.Size([16, 1, 9, 1375])\n",
      "269: torch.Size([16, 1, 9, 1375])\n",
      "270: torch.Size([16, 1, 9, 1375])\n",
      "271: torch.Size([16, 1, 9, 1375])\n",
      "272: torch.Size([16, 1, 9, 1375])\n",
      "273: torch.Size([16, 1, 9, 1375])\n",
      "274: torch.Size([16, 1, 9, 1375])\n",
      "275: torch.Size([16, 1, 9, 1375])\n",
      "276: torch.Size([16, 1, 9, 1375])\n",
      "277: torch.Size([16, 1, 9, 1375])\n",
      "278: torch.Size([16, 1, 9, 1375])\n",
      "279: torch.Size([16, 1, 9, 1375])\n",
      "280: torch.Size([16, 1, 9, 1375])\n",
      "281: torch.Size([16, 1, 9, 1375])\n",
      "282: torch.Size([16, 1, 9, 1375])\n",
      "283: torch.Size([16, 1, 9, 1375])\n",
      "284: torch.Size([16, 1, 9, 1375])\n",
      "285: torch.Size([16, 1, 9, 1375])\n",
      "286: torch.Size([16, 1, 9, 1375])\n",
      "287: torch.Size([16, 1, 9, 1375])\n",
      "288: torch.Size([16, 1, 9, 1375])\n",
      "289: torch.Size([16, 1, 9, 1375])\n",
      "290: torch.Size([16, 1, 9, 1375])\n",
      "291: torch.Size([16, 1, 9, 1375])\n",
      "292: torch.Size([16, 1, 9, 1375])\n",
      "293: torch.Size([16, 1, 9, 1375])\n",
      "294: torch.Size([16, 1, 9, 1375])\n",
      "295: torch.Size([16, 1, 9, 1375])\n",
      "296: torch.Size([16, 1, 9, 1375])\n",
      "297: torch.Size([16, 1, 9, 1375])\n",
      "298: torch.Size([16, 1, 9, 1375])\n",
      "299: torch.Size([16, 1, 9, 1375])\n",
      "300: torch.Size([16, 1, 9, 1375])\n",
      "301: torch.Size([16, 1, 9, 1375])\n",
      "302: torch.Size([16, 1, 9, 1375])\n",
      "303: torch.Size([16, 1, 9, 1375])\n",
      "304: torch.Size([16, 1, 9, 1375])\n",
      "305: torch.Size([16, 1, 9, 1375])\n",
      "306: torch.Size([16, 1, 9, 1375])\n",
      "307: torch.Size([16, 1, 9, 1375])\n",
      "308: torch.Size([16, 1, 9, 1375])\n",
      "309: torch.Size([16, 1, 9, 1375])\n",
      "310: torch.Size([16, 1, 9, 1375])\n",
      "311: torch.Size([16, 1, 9, 1375])\n",
      "312: torch.Size([16, 1, 9, 1375])\n",
      "313: torch.Size([16, 1, 9, 1375])\n",
      "314: torch.Size([16, 1, 9, 1375])\n",
      "315: torch.Size([16, 1, 9, 1375])\n",
      "316: torch.Size([16, 1, 9, 1375])\n",
      "317: torch.Size([16, 1, 9, 1375])\n",
      "318: torch.Size([16, 1, 9, 1375])\n",
      "319: torch.Size([16, 1, 9, 1375])\n",
      "320: torch.Size([16, 1, 9, 1375])\n",
      "321: torch.Size([16, 1, 9, 1375])\n",
      "322: torch.Size([16, 1, 9, 1375])\n",
      "323: torch.Size([16, 1, 9, 1375])\n",
      "324: torch.Size([16, 1, 9, 1375])\n",
      "325: torch.Size([16, 1, 9, 1375])\n",
      "326: torch.Size([16, 1, 9, 1375])\n",
      "327: torch.Size([16, 1, 9, 1375])\n",
      "328: torch.Size([16, 1, 9, 1375])\n",
      "329: torch.Size([16, 1, 9, 1375])\n",
      "330: torch.Size([16, 1, 9, 1375])\n",
      "331: torch.Size([16, 1, 9, 1375])\n",
      "332: torch.Size([16, 1, 9, 1375])\n",
      "333: torch.Size([16, 1, 9, 1375])\n",
      "334: torch.Size([16, 1, 9, 1375])\n",
      "335: torch.Size([16, 1, 9, 1375])\n",
      "336: torch.Size([16, 1, 9, 1375])\n",
      "337: torch.Size([16, 1, 9, 1375])\n",
      "338: torch.Size([16, 1, 9, 1375])\n",
      "339: torch.Size([16, 1, 9, 1375])\n",
      "340: torch.Size([16, 1, 9, 1375])\n",
      "341: torch.Size([16, 1, 9, 1375])\n",
      "342: torch.Size([16, 1, 9, 1375])\n",
      "343: torch.Size([16, 1, 9, 1375])\n",
      "344: torch.Size([16, 1, 9, 1375])\n",
      "345: torch.Size([16, 1, 9, 1375])\n",
      "346: torch.Size([16, 1, 9, 1375])\n",
      "347: torch.Size([16, 1, 9, 1375])\n",
      "348: torch.Size([16, 1, 9, 1375])\n",
      "349: torch.Size([16, 1, 9, 1375])\n",
      "350: torch.Size([16, 1, 9, 1375])\n",
      "351: torch.Size([16, 1, 9, 1375])\n",
      "352: torch.Size([16, 1, 9, 1375])\n",
      "353: torch.Size([16, 1, 9, 1375])\n",
      "354: torch.Size([16, 1, 9, 1375])\n",
      "355: torch.Size([16, 1, 9, 1375])\n",
      "356: torch.Size([16, 1, 9, 1375])\n",
      "357: torch.Size([16, 1, 9, 1375])\n",
      "358: torch.Size([16, 1, 9, 1375])\n",
      "359: torch.Size([16, 1, 9, 1375])\n",
      "360: torch.Size([16, 1, 9, 1375])\n",
      "361: torch.Size([16, 1, 9, 1375])\n",
      "362: torch.Size([16, 1, 9, 1375])\n",
      "363: torch.Size([16, 1, 9, 1375])\n",
      "364: torch.Size([16, 1, 9, 1375])\n",
      "365: torch.Size([16, 1, 9, 1375])\n",
      "366: torch.Size([16, 1, 9, 1375])\n",
      "367: torch.Size([16, 1, 9, 1375])\n",
      "368: torch.Size([16, 1, 9, 1375])\n",
      "369: torch.Size([16, 1, 9, 1375])\n",
      "370: torch.Size([16, 1, 9, 1375])\n",
      "371: torch.Size([16, 1, 9, 1375])\n",
      "372: torch.Size([16, 1, 9, 1375])\n",
      "373: torch.Size([16, 1, 9, 1375])\n",
      "374: torch.Size([16, 1, 9, 1375])\n",
      "375: torch.Size([16, 1, 9, 1375])\n",
      "376: torch.Size([16, 1, 9, 1375])\n",
      "377: torch.Size([16, 1, 9, 1375])\n",
      "378: torch.Size([16, 1, 9, 1375])\n",
      "379: torch.Size([16, 1, 9, 1375])\n",
      "380: torch.Size([16, 1, 9, 1375])\n",
      "381: torch.Size([16, 1, 9, 1375])\n",
      "382: torch.Size([16, 1, 9, 1375])\n",
      "383: torch.Size([16, 1, 9, 1375])\n",
      "384: torch.Size([16, 1, 9, 1375])\n",
      "385: torch.Size([16, 1, 9, 1375])\n",
      "386: torch.Size([16, 1, 9, 1375])\n",
      "387: torch.Size([16, 1, 9, 1375])\n",
      "388: torch.Size([16, 1, 9, 1375])\n",
      "389: torch.Size([16, 1, 9, 1375])\n",
      "390: torch.Size([16, 1, 9, 1375])\n",
      "391: torch.Size([16, 1, 9, 1375])\n",
      "392: torch.Size([16, 1, 9, 1375])\n",
      "393: torch.Size([16, 1, 9, 1375])\n",
      "394: torch.Size([16, 1, 9, 1375])\n",
      "395: torch.Size([16, 1, 9, 1375])\n",
      "396: torch.Size([16, 1, 9, 1375])\n",
      "397: torch.Size([16, 1, 9, 1375])\n",
      "398: torch.Size([16, 1, 9, 1375])\n",
      "399: torch.Size([16, 1, 9, 1375])\n",
      "400: torch.Size([16, 1, 9, 1375])\n",
      "401: torch.Size([16, 1, 9, 1375])\n",
      "402: torch.Size([16, 1, 9, 1375])\n",
      "403: torch.Size([16, 1, 9, 1375])\n",
      "404: torch.Size([16, 1, 9, 1375])\n",
      "405: torch.Size([16, 1, 9, 1375])\n",
      "406: torch.Size([16, 1, 9, 1375])\n",
      "407: torch.Size([16, 1, 9, 1375])\n",
      "408: torch.Size([16, 1, 9, 1375])\n",
      "409: torch.Size([16, 1, 9, 1375])\n",
      "410: torch.Size([16, 1, 9, 1375])\n",
      "411: torch.Size([16, 1, 9, 1375])\n",
      "412: torch.Size([16, 1, 9, 1375])\n",
      "413: torch.Size([16, 1, 9, 1375])\n",
      "414: torch.Size([16, 1, 9, 1375])\n",
      "415: torch.Size([16, 1, 9, 1375])\n",
      "416: torch.Size([16, 1, 9, 1375])\n",
      "417: torch.Size([16, 1, 9, 1375])\n",
      "418: torch.Size([16, 1, 9, 1375])\n",
      "419: torch.Size([16, 1, 9, 1375])\n",
      "420: torch.Size([16, 1, 9, 1375])\n",
      "421: torch.Size([16, 1, 9, 1375])\n",
      "422: torch.Size([16, 1, 9, 1375])\n",
      "423: torch.Size([16, 1, 9, 1375])\n",
      "424: torch.Size([16, 1, 9, 1375])\n",
      "425: torch.Size([16, 1, 9, 1375])\n",
      "426: torch.Size([16, 1, 9, 1375])\n",
      "427: torch.Size([16, 1, 9, 1375])\n",
      "428: torch.Size([16, 1, 9, 1375])\n",
      "429: torch.Size([16, 1, 9, 1375])\n",
      "430: torch.Size([16, 1, 9, 1375])\n",
      "431: torch.Size([16, 1, 9, 1375])\n",
      "432: torch.Size([16, 1, 9, 1375])\n",
      "433: torch.Size([16, 1, 9, 1375])\n",
      "434: torch.Size([16, 1, 9, 1375])\n",
      "435: torch.Size([16, 1, 9, 1375])\n",
      "436: torch.Size([16, 1, 9, 1375])\n",
      "437: torch.Size([8, 1, 9, 1375])\n"
     ]
    }
   ],
   "source": [
    "# dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "# for i, (eegs, labels) in enumerate(dataloader):\n",
    "#     print(f\"{i}:\", eegs.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T05:29:04.120996800Z",
     "start_time": "2024-02-23T05:29:03.836879900Z"
    }
   },
   "id": "4c916b19155f4de5"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.   8.2  8.4  8.6  8.8  9.   9.2  9.4  9.6  9.8 10.  10.2 10.4 10.6\n",
      " 10.8 11.  11.2 11.4 11.6 11.8 12.  12.2 12.4 12.6 12.8 13.  13.2 13.4\n",
      " 13.6 13.8 14.  14.2 14.4 14.6 14.8 15.  15.2 15.4 15.6 15.8]\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T05:14:22.537915Z",
     "start_time": "2024-02-23T05:14:22.512069100Z"
    }
   },
   "id": "8841b54738ec7e1e"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T05:13:53.775468700Z",
     "start_time": "2024-02-23T05:13:53.435049400Z"
    }
   },
   "id": "b56da2a0a2baa4a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5a7e143b42e9a015"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "Python Torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
